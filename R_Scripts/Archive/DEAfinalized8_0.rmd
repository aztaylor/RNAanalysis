```{r} 
# Load necessary libraries 
library("tximport") 
library("readr")
library("DESeq2")
library("tximportData")
library("ggplot2")
library("tximeta")
library("magrittr")
library("dplyr")
library("tidyverse")
library("ggpubr")
library("pheatmap")
library("grid")
library("ggplotify")
library("gridExtra")
library("vsn")
library("RColorBrewer")
library("PoiClaClu")
library("glmpca")
library("ggbeeswarm")
library("apeglm")
library("genefilter")
library("AnnotationHub")
library("tm") # Text mining
library("tidytext")
library("text2vec")
library("lsa")
library("topicmodels")
library("AnnotationDbi")
library("gridExtra")
library(httr)
library(jsonlite)

select <- function(...){ dplyr::select(...)}
```

```{R}
# List all of the quant.sf files
cwd <- getwd() 
par_dir <- dirname(cwd)
quant_fp <- paste(par_dir, 'quants', sep = '/') 
quant_files<- list.files(path = quant_fp, pattern = "quant.sf$", recursive = TRUE)
```

```{r}
# Extract sample names
quant_dirs <- list.files(quant_fp, 
                        pattern = "_quant$", 
                        full.names = TRUE)
sample_names <- gsub("_quant$", "", basename(quant_dirs)) 
# Correct for the FASTQ generation file included in the directory sample_names \<-
sample_names <- sample_names[1:48] # Ensure this range fits your data
```

```{r}
quant_fp
```

```{r}
# Create the txi import
txi <- tximport(files = paste(quant_fp, quant_files, sep = '/'), type = "salmon", txOut = TRUE)
```

```{r}
colnames(txi$abundance) <- sample_names
colnames(txi$counts) <- sample_names
colnames(txi$length) <- sample_names
```

```{r}
splitstrs <- strsplit(sample_names, "\\_|\\-")
string = splitstrs[[1]]
#length(splitstrs)
#splitstrs
```

```{r}
genotype <- c()
treatment <- c()
timepoint <- c()
replicate <- c()
for (i in 1:length(sample_names)){
  strippedConds = strsplit(sample_names, "\\_|\\-")
  genotype <- append(genotype, strippedConds[[i]][2])
  if (length(strippedConds[[i]]) == 4){
    treatment <- append(treatment, "Nind")
    if (strippedConds[[i]][3] == 1){
      timepoint <- append(timepoint, 5)
    } else if (strippedConds[[i]][3] == 3){
      timepoint <- append(timepoint, 18)
    }
    replicate <- append(replicate, strippedConds[[i]][4])
  }
  else if (length(strippedConds[[i]]) == 5){
    treatment = append(treatment, "ind")
    if (strippedConds[[i]][4] == 1){
      timepoint <- append(timepoint, 5)
    } else if (strippedConds[[i]][4] == 3){
      timepoint <- append(timepoint, 18)
    }
    replicate <- append(replicate, strippedConds[[i]][5])
  }
  
}
```

```{r}
samples <- sub("_L00",".",sample_names)
si_complete <-data.frame(samples=samples,
                         genotype=genotype,
                         treatment=treatment,
                         timepoint=timepoint,
                         replicate=replicate)
```
```{r}
si_complete
```
```{r}
coldata <- si_complete
coldata
coldata$names <- sub("_L00[0-9]+","",sample_names)
coldata$files <- paste(quant_fp, quant_files, sep="/")
coldata
```
```{r}
coldata <- subset(coldata, select = -replicate)
coldata
```

```{r}
se <- tximeta(coldata) #SummarizedExperiment Object
```
```{r}
if (!is.factor(se$treatment)){
  se$treatment <- factor(se$treatment)
}
if (!is.factor(se$timepoint)){
  se$timepoint <- factor(se$timepoint)
}
if (!is.factor(se$genotype)){
  se$genotype <- factor(se$genotype)
}
se$treatment %<>% relevel("Nind") # Nind is the base line to compare to. 
se$timepoint %<>% relevel("5")
se$genotype %<>% relevel("WT")
```

```{r}
design =~ genotype*timepoint*treatment #~ genotype + timepoint + treatment + genotype:timepoint + genotype:treatment + genotype:timepoint:treatment

dds <- DESeqDataSet(se, design = design)
#dds <- collapseReplicates(dds, dds$samples, dds$replicate)
meanSdPlot(assay(dds), ranks = FALSE)

log.cts.one <- log2(assay(dds) + 1)
meanSdPlot(log.cts.one, ranks = FALSE)
```
# Run Prefiltering because DESeqDataSet contains several rows that contain zeros or small amounts. Prefiltering will remove these and allow for increased speed when running functions.
```{r}
nrow(dds)
smallestGroupSize <- 4
keep <- rowSums(counts(dds) >= 10) >= smallestGroupSize
dds <- dds[keep,]
nrow(dds)
```
# Lets look at the simulated data of what the counts should look like if they follow a poisson distribution.
```{r}
lambda <- 10^seq(from = -1, to = 2, length = 1000)
cts <- matrix(rpois(1000*100, lambda), ncol = 100)
meanSdPlot(cts, ranks = FALSE)

log.cts.one <- log2(cts + 1)
meanSdPlot(log.cts.one, ranks = FALSE)
```
# Now we will compare the variance stabilizing transformation (VST), regularized-logarithm transformation (rlog), and log2 transformation.
```{r}
vsd <- varianceStabilizingTransformation(dds, blind = FALSE)
#head(assay(vsd), 3)
```
```{r}
rld <- rlog(dds, blind = FALSE)
#head(assay(rld), 3)
```
```{r}
dds <- estimateSizeFactors(dds)

df <- bind_rows(
  as_data_frame(log2(counts(dds, normalized=TRUE)[, c(7,11)]+1)) %>%
         mutate(transformation = "log2(x + 1)"),
  as_data_frame(assay(vsd)[, c(7,11)]) %>% mutate(transformation = "vst"),
  as_data_frame(assay(rld)[, c(7,11)]) %>% mutate(transformation = "rlog"))
  
colnames(df)[1:2] <- c("x", "y")  

lvls <- c("log2(x + 1)", "vst", "rlog")
df$transformation <- factor(df$transformation, levels=lvls)

ggplot(df, aes(x = x, y = y)) + geom_hex(bins = 80) +
  coord_fixed() + facet_grid( . ~ transformation) 
```
# Now we can look at the euclidean distance between the sample to check for similarity between them.
```{r}
sampleDists <- dist(t(assay(dds)))
```
# And Create a heatmap to visualize.
```{r}
sampleDistMatrix <- as.matrix( sampleDists )
rownames(sampleDistMatrix) <- paste( dds$treatment, dds$genotype, dds$timepoint, sep = "-" )
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows = sampleDists,
         clustering_distance_cols = sampleDists,
         col = colors)
```
# Do the same using a poisson distance.
```{r}
poisd <- PoissonDistance(t(counts(dds)))
samplePoisDistMatrix <- as.matrix( poisd$dd )
rownames(samplePoisDistMatrix) <- paste( dds$treatment, dds$genotype, sep=" - " )
colnames(samplePoisDistMatrix) <- NULL
pheatmap(samplePoisDistMatrix,
         clustering_distance_rows = poisd$dd,
         clustering_distance_cols = poisd$dd,
         col = colors)
```
# Finally with a PCA.
```{r}
plotPCA(vsd, intgroup = c("treatment", "genotype", "timepoint"))
```
```{r}
pcaData <- plotPCA(vsd, intgroup = c( "treatment", "genotype", "timepoint"), returnData = TRUE)
pcaData
percentVar <- round(100 * attr(pcaData, "percentVar"))
ggplot(pcaData, aes(x = PC1, y = PC2, color = treatment, shape = genotype)) +
  geom_point(size =3, mapping = aes(alpha = timepoint)) +
  xlab(paste0("PC1: ", percentVar[1], "% variance")) +
  ylab(paste0("PC2: ", percentVar[2], "% variance")) +
  coord_fixed() +
  ggtitle("PCA with VST data")
```
# Finally, finally with a generalized PCA (GLM-PCA)
```{r}
gpca <- glmpca(counts(dds), L=2)
gpca.dat <- gpca$factors
gpca.dat$treatment <- dds$treatment
gpca.dat$genotype <- dds$genotype
gpca.dat$timepoint <- dds$timepoint

ggplot(gpca.dat, aes(x = dim1, y = dim2, color = treatment, shape = genotype)) + geom_point(size =3, mapping = aes(aplha = timepoint)) + coord_fixed() + ggtitle("glmpca - Generalized PCA")
```
# multidimensional scaling plot.
```{r}
mds <- as.data.frame(colData(vsd))  %>%
         cbind(cmdscale(sampleDistMatrix))
ggplot(mds, aes(x = `1`, y = `2`, color = treatment, shape = genotype, alpha = timepoint)) +
  geom_point(size = 3) + coord_fixed() + ggtitle("MDS with VST data")
```
```{r}
dds <- DESeq(dds)
res <- results(dds, name = "genotype_s4_vs_WT")
topGene <- rownames(res)[which.max(res$log2FoldChange)]
plotCounts(dds, gene = topGene, intgroup=c("treatment", "genotype", "timepoint"))
```

```{r}
geneCounts <- plotCounts(dds, gene = topGene, intgroup = c("treatment","genotype", "timepoint"),
                         returnData = TRUE)
ggplot(geneCounts, aes(x = timepoint, y = count, color = genotype, shape = treatment)) +
  scale_y_log10() +  geom_beeswarm(cex = 3)
```

```{r}
ggplot(geneCounts, aes(x = timepoint, y = count, color = genotype, group = interaction(genotype, treatment), shape = treatment)) +
  scale_y_log10() + geom_point(size = 3) + geom_line()
```


```{r, fig.height=20, fig.width=10}}
font = "Helvetica"
timep = "5"
ntopgenes = 100
pthreshold = 0.1
design =~ genotype + treatment + timepoint + genotype:treatment + genotype:timepoint + treatment:timepoint + genotype:treatment:timepoint
#design =~ genotype + timepoint + treatment

dds <- DESeqDataSet(se, design = design)
dds <- DESeq(dds)
resultsNames(dds)

# Assuming 'rld' is your rlog-transformed data (or use vst()), filter 
# for the top 50.
gene_variance <- apply(assay(rld), 1, var)  # Variance per gene

# Sort genes by variance (descending) and take top 50
top_genes <- names(sort(gene_variance, decreasing = TRUE)[1:50])
# Now create the dataframe for the heatmap

# Example: Extract and combine results for all 12 groups
heatmap_data <- bind_rows(
  ###########################################################
  # Baseline effect with induction
  ###########################################################
  
  # Baseline (WT -treatment 5hr)
  results(dds, name = "Intercept") %>% 
    as.data.frame() %>% 
    rownames_to_column("Gene") %>%
    mutate(genotype = "WT", treatment = "ind", timepoint = timep, Comparison = "Baseline"
           ),
  
  ##########################################################################
  # Plasmid (non-functional) effect with and without induction and leakyniss
  ##########################################################################

  # NC -treatment 5hr (non-functional effector leakiness)
  results(dds, list(
  c("genotype_NC_vs_WT", "treatment_ind_vs_Nind",
    "genotypeNC.treatmentind", "timepoint_18_vs_5",
    "genotypeNC.timepoint18", "treatmentind.timepoint18",
    "genotypeNC.treatmentind.timepoint18")
    )) %>% 
    as.data.frame() %>% 
    rownames_to_column("Gene") %>%
    mutate(genotype = "NC", treatment = "ind", timepoint = timep, Comparison = "NC_IPTG_WT"
           ),
  
  ###########################################################
  # Functional effector with and without induction
  ###########################################################
    # s4 +treatment 18
  results(dds, contrast = list(
    c("genotype_s4_vs_WT", "treatment_ind_vs_Nind", "genotypes4.treatmentind", "timepoint_18_vs_5",
      "genotypes4.timepoint18", "treatmentind.timepoint18",
      "genotypes4.treatmentind.timepoint18"))) %>% 
    as.data.frame() %>% 
    rownames_to_column("Gene") %>%
    mutate(genotype = "s4", treatment = "ind", timepoint = timep, Comparison = "s4_IPTG_T2"
           ),
    
) %>%
  select(Gene, genotype, treatment, timepoint, log2FoldChange, padj) %>%
  mutate(
    sig_star = case_when(
      padj < 0.001 ~ "***",
      padj < 0.01 ~ "**",
      padj < 0.1 ~ "*",
      TRUE ~ ""
    ),
    # Force factor levels for plotting
    genotype = factor(genotype, levels = c("WT", "NC", "s4")),
    treatment = factor(treatment, levels = c("Nind", "ind")),
    timepoint = factor(timepoint, levels = c("5", "18"))
  )

heatmap_data <- heatmap_data %>%
  dplyr::filter(
    !if_any(c(log2FoldChange, padj), is.na),  # Keep rows where these cols are non-NA
    padj < pthreshold  # Keep rows where these cols are non-zero
    )

# Set symmetric color limits based on max absolute LFC
lfc_limit <- max(abs(heatmap_data$log2FoldChange), na.rm = TRUE) * 1.1
# Calculate optimal dimensions based on your data
n_genes <- length(unique(heatmap_data$Gene))
n_conditions <- length(unique(heatmap_data$GenoType))

# Set your target dimensions (in inches)
fig_width <- 10 # Your specified width
fig_height <- 50 # Your specified height

# Calculate optimal tile aspect ratio
tile_aspect_ratio <- (fig_height/fig_width)*(n_conditions/n_genes)

induction_order <- heatmap_data %>%
  # Filter to only the groups we want to compare
  filter(genotype %in% c("s4", "NC"), treatment == "ind", timepoint == timep) %>%
  # For each gene, calculate the difference between s4 and NC
  group_by(Gene) %>%
  summarize(
    induction_effect = log2FoldChange[genotype == "s4"] - log2FoldChange[genotype == "NC"]
  ) %>%
  # Sort by absolute induction effect
  arrange(desc(abs(induction_effect))) %>%
  pull(Gene)


rRNAgenes <- c("rrl*", "rrs*")
divergence_data <- heatmap_data %>%
  filter(
    (genotype == "WT" & treatment == "Nind" & timepoint == timep) |
    (genotype == "NC" & treatment == "ind" & timepoint == timep) |
    (genotype == "s4" & treatment == "ind" & timepoint == timep) | 
    (!grepl(paste(rRNAgenes, collapse="|"), Gene, ignore.case = TRUE))
  ) %>%
  # Spread to wide format
  pivot_wider(
    id_cols = Gene,
    names_from = genotype,
    values_from = log2FoldChange,
  ) %>%
  # Calculate divergence metrics
  mutate(
    s4_vs_WT = `s4` - `WT`,  # Absolute difference from WT baseline
    s4_vs_NC = `NC` - `WT`,  # Absolute difference from NC control
    combined_divergence = abs(s4_vs_WT - s4_vs_NC)  # Total divergence score
  ) %>%
  arrange(desc(combined_divergence)) # Sort by highest divergence

# Result contains:
# Gene | WT | NC | s4 | s4_vs_WT | s4_vs_NC | combined_divergence

# Sort data base of the divergence data order.
top_n_genes <- divergence_data %>%
  slice_head(n = ntopgenes) %>%  # Get first 20 rows (already sorted by descending divergence)
  pull(Gene)  # Extract as character vector

# Alternative with explicit sorting:
top_n_genes <- divergence_data %>%
  arrange(desc(combined_divergence)) %>%  # Ensure descending order
  head(ntopgenes) %>%
  pull(Gene)

gene_order <- top_n_genes

heatmap_data_sorted <- heatmap_data %>%
  filter(Gene %in% gene_order) %>%
  mutate(Gene = factor(Gene, levels = gene_order, exclude=NULL)) %>%
  arrange(Gene, .by_group = TRUE) 

# 1. Calculate WT baseline values (per gene)
wt_baseline <- heatmap_data_sorted %>%
  filter(genotype == "WT",
        Gene %in% gene_order) %>%
  dplyr::select(Gene, treatment, timepoint, wt_l2fc = log2FoldChange)

# 2. Create relative fold changes (s4-WT and NC-WT)
heatmap_relative <- heatmap_data_sorted %>%
  filter(genotype %in% c("NC", "s4")) %>%  # Exclude WT
  inner_join(wt_baseline, by = c("Gene", "treatment", "timepoint")) %>%
  mutate(
    relative_l2fc = log2FoldChange - wt_l2fc,  # Calculate difference from WT
    genotype = paste0(genotype, "-WT"),  # Update genotype labels
    Gene = factor(Gene, levels = gene_order, exclude=NULL)  # Ensure consistent ordering)
  ) %>%
  select(-log2FoldChange, -wt_l2fc) %>%  # Remove original columns
  rename(log2FoldChange = relative_l2fc)  # Rename for consistency

# 3. (Optional) Combine with other annotations
heatmap_relative <- heatmap_relative %>%
  left_join(
    heatmap_data %>% select(Gene, genotype, sig_star),  # Keep significance stars
    by = c("Gene", "genotype")
  )
  
ggplot(heatmap_data_sorted, aes(
    x = interaction(genotype, treatment, timepoint, sep = " ", lex.order = TRUE), 
    y = fct_rev(Gene))
) +
  scale_y_discrete(limits = levels(rev(gene_order))) +
  geom_tile(aes(fill = log2FoldChange)) +
  #facet_grid(~ timepoint, scales = "free_x") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    #limits = c(-3, 3)  # Adjust based on your data
  ) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
    strip.text = element_text(size = 10)
  ) +
  labs(
    x = "Strain + IPTG",
    y = "Gene",
    fill = expression(atop(log[2],"Fold Change")) 
  )
```
# Now let's create a heatmap of the relative fold changes (subtracting the WT baseline).
```{r, fig.width=5, fig.height=10}
ggplot(heatmap_relative, aes(
    x = interaction(genotype, treatment, timepoint, sep = " ", lex.order = TRUE), 
    y = fct_rev(Gene))
) +
  scale_y_discrete(limits = rev(top_n_genes)) +
  geom_tile(aes(fill = log2FoldChange)) +
  #facet_grid(~ timepoint, scales = "free_x") +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1, size = 8),
    strip.text = element_text(size = 10)
  ) +
  labs(
    x = "Strain + IPTG",
    y = "Gene",
    fill = expression(atop(log[2],"Fold Change")) 
                            
  )
```


```{r}
genes <- c(
  # SOS responsive, LexA-dependent genes
  "polB", "recA", "recN", "sbmC", "ssb", "sulA", "uvrA", "uvrB",
  
  # LexA-independent genes
  "dnaA", "nrdA", "nrdB", "cydA", "tdcB", "tdcC", "cstA", "sspA", "sspB",
  "deoA", "deoB", "deoC", "dnaG", "gmk", "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG",
  
  # Anaerobic induction
  "cydA", "tdcB", "tdcC",
  
  # Starvation induction
  "cstA", "sspA", "sspB",
  
  # Other regulation
  "deoA", "deoB", "deoC", "dnaG", "gmk", "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG",
  
  # Transcription-related genes
  "fliA", "fliZ", "greA", "rho", "rpoA", "rpoD",
  
  # Translation-related genes
  "fusA", "infA", "rnpA", "rpmB", "rpmG", "rpmH", "rpsG", "rpsU", "trmD",
  
  # Hypothetical or unknown function genes
  "yabO", "yagP", "ychB", "ydiY", "yebE", "yebF", "yeeN", "yfgB", "yfhN",
  "yfhO", "yghB", "yhjG", "yi52–10", "yibB", "yjeS", "ylaC", "yleA"
)

# Create a data frame for norfloxacin expression values from Table 1
nor_df <- data.frame(
  Gene = c("polB", "recA", "recN", "sbmC", "ssb", "sulA", "uvrA", "uvrB", "dnaA", "nrdA", "nrdB",
           "fliA", "fliZ", "greA", "rho", "rpoA", "rpoD", "deoA", "deoB", "deoC", "dnaG", "gmk",
           "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG", "yabO", "yagP", "ychB", "ydiY", "yebE",
           "yfgB", "yfhN", "yfhO", "yghB", "yhjG", "yi52–10", "yibB", "yjeS", "ylaC", "yleA"),
  
  Nor_0.5_MIC = c(0.93, 3.79, 2.36, 1.14, 0.85, 1.36, 1.49, 1.25, 0.95, 1.48, 1.27, 1.18, 0.69, 
                  0.99, 1.04, 0.86, 0.99, 1.12, 1.21, 1.19, 0.98, 1.03, 0.97, 0.76, 0.84, 1.02, 
                  0.98, 1.04, 0.93, 0.97, 0.86, 0.86, 1.09, 0.97, 0.93, 0.83, 0.98, 0.87, 1.02, 
                  1.00, 0.96, 0.85, 0.94),
  
  Nor_1_MIC = c(1.22, 6.37, 3.65, 2.78, 1.06, 2.84, 2.12, 1.23, 1.25, 1.30, 1.08, 0.88, 0.71, 
                0.97, 1.02, 0.80, 2.05, 1.28, 1.29, 1.15, 1.11, 1.06, 0.88, 1.00, 0.85, 0.92, 
                0.92, 0.94, 0.93, 0.86, 0.97, 0.88, 0.97, 0.89, 0.89, 0.75, 0.91, 0.69, 0.86, 
                0.94, 0.81, 0.92, 0.81),
  
  Nor_2_MIC = c(1.31, 11.10, 4.82, 5.59, 1.00, 4.50, 1.97, 1.39, 1.16, 1.65, 1.48, 0.98, 0.39, 
                1.93, 0.67, 0.75, 4.64, 1.32, 3.25, 1.42, 1.69, 2.35, 0.84, 0.97, 0.81, 1.03, 
                1.21, 1.03, 1.07, 1.03, 1.62, 0.92, 1.01, 1.62, 1.27, 1.74, 1.08, 0.94, 1.03, 
                1.53, 1.21, 2.39, 1.43),
  
  Nor_33_MIC = c(4.84, 47.61, 16.14, 0.93, 2.94, 6.44, 21.63, 3.40, 3.65, 8.92, 3.96, 2.81, 
                 2.72, 4.35, 3.23, 0.35, 7.85, 2.25, 4.09, 5.24, 4.69, 2.69, 7.07, 5.09, 2.64, 
                 4.54, 5.11, 7.41, 3.71, 2.83, 5.21, 3.46, 1.52, 2.09, 3.18, 4.44, 4.08, 2.90, 
                 2.35, 3.18, 2.78, 3.18, 3.56),
  
  Nor_133_MIC = c(4.27, 47.55, 11.69, 0.83, 3.75, 4.98, 46.35, 4.71, 3.68, 5.17, 2.07, 4.44, 
                  5.25, 3.86, 4.09, 0.22, 9.45, 1.75, 6.07, 6.83, 5.73, 3.95, 8.85, 6.02, 3.64, 
                  5.05, 5.35, 9.76, 3.61, 3.56, 3.63, 2.44, 1.66, 3.01, 3.26, 5.28, 6.82, 4.93, 
                  3.43, 4.68, 3.98, 6.25, 6.25)
)

# View the data frame
rownames(nor_df)<-nor_df$Gene
nor_df <- log(nor_df[,-1], 2)
```

```{r}
heatmap_data_comp <- heatmap_data %>%
  filter(genotype == "s4", treatment == "ind", timepoint == timep) %>%
  select(Gene, log2FoldChange) %>%
  remove_rownames() %>%
  column_to_rownames(var="Gene")

gene_overlap <- rownames(nor_df) %in% rownames(heatmap_data_comp)
subset_heatmap <- heatmap_data_comp[rownames(heatmap_data_comp) %in% rownames(nor_df), , drop=FALSE]
subset_nor_df <- nor_df[rownames(nor_df) %in% rownames(heatmap_data_comp), , drop=FALSE]

subset_df <- Reduce(function(x,y) merge(x,y, by='row.names', all=T), list(subset_heatmap, subset_nor_df))
rownames(subset_df)<-subset_df$Row.names
subset_df <- subset_df[, -1]
```

```{r}
var_nor <- apply(nor_df, MARGIN=1, FUN= var)
var_nor_df = data.frame(Var=var_nor)
rownames(var_nor_df) <- rownames(nor_df)

varNor_l2f_df <- nor_df[order(var_nor_df$Var, decreasing=FALSE), , drop=FALSE]
varNor_l2f_df

varNor_l2f_df <- varNor_l2f_df %>% rownames_to_column() %>% gather(colname, value, -rowname)

varNor_l2f_df <- varNor_l2f_df %>% mutate(colname = case_when(
  colname == "Nor_0.5_MIC" ~ "0.5\nMIC",
  colname == "Nor_1_MIC" ~ "1\nMIC",
  colname == "Nor_2_MIC" ~ "2\nMIC",
  colname == "Nor_33_MIC" ~ "33\nMIC",
  colname == "Nor_133_MIC" ~ "133\nMIC"
))

columns <- c("Norfloxacin\n0.5MIC", "Norfloxacin\n1MIC", "Norfloxacin\n2MIC", "Norfloxacin\n33MIC", "Norfloxacin\n133MIC")
varNor_l2f_df$colname <- factor(varNor_l2f_df$colname, levels=unique(varNor_l2f_df$colname))
varNor_l2f_df
varNor_l2f_df$rowname<-factor(varNor_l2f_df$rowname, levels=unique(varNor_l2f_df$rowname))

fsize = 7

nor_top_hits <- ggplot(varNor_l2f_df, aes(x=colname, y=rowname, fill=value)) + 
  geom_tile() +
  #scale_fill_distiller(palette="RdGy") +
  theme(axis.text.x = element_text(angle=45, family=font, size=fsize)) +
  theme(axis.text.y = element_text(family=font, size=fsize)) +
  coord_cartesian(clip = "off") +  
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  #scale_x_discrete(labels = columns) +
  xlab("Treatment") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change")) )
print(nor_top_hits)
ggsave("../Figures/NorTopHits.png", nor_top_hits, dpi=600)
```

```{r}
x_ann1 <- textGrob("timepointpoint: 5hr.", gp=gpar(fontsize=10))
x_ann2 <- textGrob("timepointpoint: 18hr.", gp=gpar(fontsize=10))

subset_sum_df<- data.frame(sort(rowSums(abs(subset_df)), decreasing=FALSE))
subset_df <- subset_df[rownames(subset_sum_df),]

subset_df2 <- subset_df %>% rownames_to_column() %>% gather(colname, value, -rowname)

subset_df2 <- subset_df2 %>% mutate(group = case_when(
  grepl("log2FoldChange", colname)~"dCasRx",
  grepl("Nor", colname)~"Norfloxacin"
))
subset_df2 <- subset_df2 %>% mutate(label = case_when(
  colname == "log2FoldChange" ~ "dCasRx\n5hr",
  colname == "Nor_0.5_MIC" ~ "Nofloxacin\n0.5MIC",
  colname == "Nor_1_MIC" ~ "Nofloxacin\n1MIC",
  colname == "Nor_2_MIC" ~ "Nofloxacin\n2MIC",
  colname == "Nor_33_MIC" ~ "Nofloxacin\n33MIC",
  colname == "Nor_133_MIC" ~ "Nofloxacin\n133MIC"
))

subset_df2$label <- factor(subset_df2$label, levels=c("dCasRx\n5hr", "Nofloxacin\n0.5MIC", "Nofloxacin\n1MIC", "Nofloxacin\n2MIC", "Nofloxacin\n33MIC", "Nofloxacin\n133MIC"))
subset_df2$colname  <- factor(subset_df2$colname, levels=c("Targeting5hr", "Nor_0.5_MIC", "Nor_1_MIC", "Nor_2_MIC", "Nor_33_MIC", "Nor_133_MIC"))
subset_df2$rowname <- factor(subset_df2$rowname, levels=unique(subset_df2$rowname))

CasRx_df <- subset_df2 %>% filter(group=="CasRx")
Nor_df <- subset_df2 %>% filter(group=="Norfloxacin")

trt_comp <- ggplot(subset_df2, aes(x=label, y=rowname, fill=value)) + 
  geom_tile() +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  theme(axis.text.x = element_text(angle=45, hjust = 1, family=font)) +
  #theme( plot.margin=unit(c(2,2,2,2), "lines")) +
  #annotation_custom(x_ann1, xmin=1.5, xmax=1, ymin=6, ymax=6) +
  #annotation_custom(x_ann2, xmin= 3.5, xmax=3.5, ymin=6, ymax=6)+
  #facet_wrap( ~group, scales="free_x" ) +
  #scale_x_discrete(labels = c("Non-Targeting gRNA", "Non-Targeting gRNA", "Targeing gRNA", "Targeting gRNA", "Norfloxacin 0.5MIC", "Norfloxacin 1MIC", "Norfloxacin 2MIC", "Norfloxacin 33MIC", "Norfloxacin 133MIC")) +
  coord_cartesian(clip = "off") +
  xlab("Treatment") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change"))
)
print(trt_comp)  
ggsave("../Figures/trt_compNC_vs_s4.png", trt_comp, dpi=600)
```
# Pull the gene functions from Uniprot.
```{r}
get_go_terms <- function(entry) {
  if (!is.null(entry$dbReferences) && 
      "type" %in% names(entry$dbReferences)) {
    go_terms <- entry$dbReferences[entry$dbReferences$type == "GO", ]
    if (nrow(go_terms) > 0) {
      return(go_terms[, c("id", "properties.term")])
    }
  }
  return(data.frame(id=character(), term=character())) # Empty df if no GO terms
}

library(UniProt.ws)
up <- UniProt.ws(taxId = 83333)  # Automatically loads latest UniProt data

# Fetch GO terms for TP53, BRCA1, EGFR
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
library(org.EcK12.eg.db)

# 1. Get ALL UniProt IDs for your genes (including Trembl)
genes <- as.character(heatmap_data_sorted$Gene)
all_genes <- data.frame(Gene = heatmap_data_sorted$Gene) %>%
  mutate(ENTREZID = mapIds(
    org.EcK12.eg.db,
    keys = genes,
    keytype = "SYMBOL",
    column = "ENTREZID",
    multiVals = "first"
  ))

go_df <- AnnotationDbi::select(up, 
                          keys = all_genes$ENTREZID, 
                          columns = c("go"), 
                          keytype = "GeneID"
                          )

function_df <- all_genes %>%
  inner_join(go_df, by = c("ENTREZID" = "From")) %>%
  rename(UniProtID = Entry, Functions = colnames(.)[[4]]) %>%
  # Split into individual term-GO pairs
  mutate(pairs = str_split(Functions, ";\\s*")) %>%
  unnest(pairs) %>%
  filter(pairs != "") %>%
  # Extract clean components
  mutate(
    function_term = str_replace_all(pairs, "\\s*\\[[^]]+\\]", "") %>% str_trim(),
    go_id = str_extract(pairs, "GO:\\d+")
  ) %>%
  # Group by gene and combine terms
  group_by(Gene, UniProtID, ENTREZID) %>%
  summarize(
    Functions = paste(unique(function_term[function_term != ""]), collapse = ", "),
    GO = paste(unique(go_id[!is.na(go_id)]), collapse = ", "),
    .groups = "drop"
  ) %>%
  # Remove any rows with empty GO terms
  filter(GO != "")

write.csv(function_df, file = "../Data/Function_Descriptions.csv", row.names = TRUE)
# Now let's analyze the functions and cluster using natural language process approaches.
```
# Create corpus based of function.
```{r}
function_df <- read.csv("../Data/Function_Descriptions.csv", row.names = 1)
# Check the exact gene name in UniProt first
function_descriptions <- function_df$Functions

corpus <- Corpus(VectorSource(function_descriptions)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, paste0("\\b", stopwords("en"), "\\b")) %>%
  tm_map(stemDocument)

function_df$function_term <- sapply(corpus, as.character)
function_df <- na.omit(function_df)
function_df <- subset(function_df, select = - function_term)
Master_df <- inner_join(function_df, heatmap_data, by = "Gene")
```
# Vectorize the corpus using A Bag-of-Words with TF-IDF weighting and Word Embeddings via Glove or Word2Vec.
```{r}
# A Bag-of-Words with TF-IDF weighting.
k<-5

BoW_dtm <- function_df %>%
  unnest_tokens(word, Functions) %>%
  count(Gene, word) %>%
  cast_dtm(Gene, word, n) 

library(tm)
library(topicmodels)

# 1. Start with your original BoW_dtm (57x794)
#    Ensure it's a valid DocumentTermMatrix
stopifnot(inherits(BoW_dtm, "DocumentTermMatrix"))

# 2. Remove sparse terms FIRST (less aggressive threshold)
BoW_lda_dtm <- removeSparseTerms(BoW_dtm, sparse = 0.999)  # Start with 0.95, adjust as needed
cat("After term removal:", dim(BoW_lda_dtm), "\n")  # Should show [57 x reduced_terms]

# 3. Then remove empty documents
BoW_rowTotals <- rowSums(as.matrix(BoW_lda_dtm))
BoW_lda_dtm <- BoW_lda_dtm[BoW_rowTotals > 0, ]
cat("After doc removal:", dim(BoW_lda_dtm), "\n")  # e.g., [55 x 120]

# 4. Run LDA only if documents remain
if(nrow(BoW_lda_dtm) > 0) {
  BoW_lda_model <- LDA(BoW_lda_dtm, k = k, control = list(seed = 123))
} else {
  stop("All documents were filtered out! Adjust sparse term threshold.")
}

it <- itoken(function_df$Functions, 
             ids = function_df$Gene,
             tokenizer = word_tokenizer,
             progressbar = TRUE)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
t2v_dtm <- create_dtm(it, vectorizer)

rownames(BoW_dtm) <- function_df$Gene[1:nrow(BoW_dtm)]

# 4. Verify
print(class(t2v_dtm))  # Should show "dgCMatrix" "dsparseMatrix" "generalMatrix"
print(dim(t2v_dtm))    # Should show [documents x terms]

rownames(t2v_dtm) <- function_df$Gene[1:nrow(t2v_dtm)]
colnames(t2v_dtm) <- vocab$term

# 1. Verify input data
cat("\n=== Input Data ===\n")
print(head(function_df$function_term))
print(length(function_df$function_term))

# 2. Check iterator
it <- itoken(function_df$Functions)
cat("\n=== Iterator ===\n")
print(class(it))

# 3. Check vocabulary
vocab <- create_vocabulary(it)
cat("\n=== Vocabulary ===\n")
print(dim(vocab))

# 4. Create and inspect DTM
t2v_dtm <- create_dtm(it, vocab_vectorizer(vocab))
cat("\n=== Final DTM ===\n")
print(class(t2v_dtm))
print(dim(t2v_dtm))
print(object.size(t2v_dtm))

t2v_dtm <- t2v_dtm[rowSums(t2v_dtm) > 0, ] # Remove empty rows
```
```{r}
empty_rows <- which(rowSums(as.matrix(BoW_dtm)) == 0)
function_df[empty_rows, c("Gene", "ENTREZID")]
```

# Try different clustering methods.
```{r}
# k-means clustering
k <- 5
BoW_kmeans_res <- kmeans(as.matrix(BoW_dtm), centers = k)
#t2v_kmeans_res <- kmeans(as.matrix(t2v_dtm), centers = k)
function_df$BoW_k_means_cluster <- as.factor(BoW_kmeans_res$cluster)
#function_df$t2v_k_means_cluster <- as.factor(t2v_kmeans_res$cluster)

# Hierarchical clustering
# Use the cosine similarity which is ideal for text data.
BoW_cosine_matrix <- as.matrix(BoW_dtm)

BoW_cosine_sim <- lsa::cosine(t(BoW_cosine_matrix))

BoW_cosine_dist <- as.dist(1 - BoW_cosine_sim)

BoW_hclust <- hclust(BoW_cosine_dist, method = "ward.D2")


# function_df <- function_df %>%
#   mutate(BoW_hclust_cluster = as.factor(cutree(BoW_hclust, k = k)),
#          t2v_hclust_cluster = as.factor(cutree(t2v_hclust, k = k)),
#          BoW_Dendro_Order = order.dendrogram(as.dendrogram(BoW_hclust)),
#          t2v_Dendro_Order = order.dendrogram(as.dendrogram(t2v_hclust)))

t2v_lda_dtm <- DocumentTermMatrix(t2v_dtm, control = list(weighting = weightTf))
t2v_lda_dtm <- removeSparseTerms(t2v_lda_dtm, 0.9999)
t2v_rowTotals <- apply(t2v_lda_dtm, 1, sum)
t2v_lda_dtm <- t2v_lda_dtm[t2v_rowTotals > 0, ]
t2v_lda_model <- LDA(t2v_lda_dtm, k = k, control = list(seed = 123))

# Note that the above commented out code does not work because of the way text2vec stores it's dtm. 
# compared to the tm library, t2v creates a raw term-count matrix, lacking explicit storage of wieghting attributes,
# using simple counts that are equivelent to tm:weightTf, and using a more memory efficient sparse matrix format.
# To fix this we could either covert to a tm DTM or use text2vec directly. Additionally we could add a weigting attribute 
# to the t2v dtm using 
#    attr(t2v_dtm, "weighting") <- c("term frequency", "tf")
#    class(t2v_dtm) <- c("DocumentTermMatrix", "simple_triplet_matrix")
# We'll use t2v directly for now.
cat("LDA documents:", nrow(BoW_lda_model@gamma), "\n")  # Should match:
cat("DTM documents:", nrow(BoW_lda_dtm), "\n")          # Should match:
cat("Dataframe rows:", nrow(function_df), "\n")          # The mismatch

# Now do verification
Matrix::nnzero(t2v_lda_dtm)

# Inspect first document

# 1. Check matrix type
print(class(t2v_dtm))  # Should show "dgCMatrix"

# 2. Inspect first non-empty document
# 1. Find first non-empty document
nonzero_docs <- which(rowSums(t2v_dtm) > 0)[1]
print(paste("First non-empty gene at position:", nonzero_docs))

# Complete diagnostics.
# 1. Check current object
cat("Object type:", class(t2v_dtm), "\n")
cat("Dimensions:", if(is.null(dim(t2v_dtm))) "vector" else dim(t2v_dtm), "\n")

# 2. If corrupted, recreate
if(!inherits(t2v_dtm, "dgCMatrix")) {
  warning("Recreating DTM...")
  t2v_dtm <- create_dtm(itoken(your_text_data), 
                     vocab_vectorizer(create_vocabulary(itoken(your_text_data))))
}

# 3. Safe inspection
if(nrow(t2v_dtm) > 0) {
  doc_index <- which(rowSums(t2v_dtm) > 0)[1]
  sparse_slice <- t2v_dtm[doc_index, , drop = FALSE]
  
  cat("\nFirst non-empty doc:", rownames(t2v_dtm)[doc_index], "\n")
  print(data.frame(
    term = colnames(t2v_dtm)[sparse_slice@i + 1],
    count = sparse_slice@x
  ))
} else {
  warning("No documents with terms found!")
}

doc_index <- which(rowSums(t2v_dtm) > 0)[1]

# Proper sparse matrix access
#term_indices <- t2v_dtm[doc_index, , drop = FALSE]@i + 1  # Note drop=FALSE
#term_counts <- t2v_dtm[doc_index, , drop = FALSE]@x

#data.frame(
 # term = colnames(t2v_dtm)[term_indices],
  #count = term_counts
#)

#function_df$BoW_topic_model <- topics(BoW_lda_model)
```
# Now visualize
```{r}
# Extract top 10 terms per topic
BoW_top_terms <- tidy(BoW_lda_model, matrix = "beta") %>%  # From tidytext
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup()

t2v_top_terms <- tidy(t2v_lda_model, matrix = "beta") %>%  # From tidytext
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup()

# Plot
plots <- list()
plots[[1]] <- ggplot(BoW_top_terms, aes(x = reorder(BoW_top_terms$term, BoW_top_terms$beta), y = beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  labs(title = "Bag of Words\nTop 10 Terms per Topic", x = "Term", y = "Beta (Probability)")

plots[[2]] <- ggplot(t2v_top_terms, aes(x = reorder(t2v_top_terms$term, beta), y = beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  labs(title = "text2vec\nTop 10 Terms per Topic", x = "Term", y = "Beta (Probability)")


library(patchwork)
combined_plot <- wrap_plots(plots, ncol = 2) +
  plot_annotation(title = "Comparison of Gene Sorting Methods") &
  theme_minimal()
print(combined_plot)
```
# Dertermine the top contributing tokens for each cluster.
```{r, fig.align="center, echo= FALSE, fig.width=5, fig.height=20}}
# Assuming:
# - `clusters` is a vector of cluster assignments (e.g., from kmeans$cluster)
# - `dtm` is your Document-Term Matrix (sparse or dense)

# Convert DTM to a dataframe with cluster labels
term_importance <- as.data.frame(as.matrix(BoW_dtm)) %>%
  mutate(cluster = function_df$BoW_k_means_cluster) %>%               # Add cluster labels
  group_by(cluster) %>%
  summarise(across(everything(), mean)) %>%    # Mean term frequency per cluster
  pivot_longer(-cluster, names_to = "term", values_to = "mean_tfidf") %>%
  group_by(cluster) %>%
  slice_max(mean_tfidf, n = 10) %>%           # Top 10 terms per cluster
  arrange(cluster, desc(mean_tfidf))

# Visualize
cluster_comp <- ggplot(term_importance, aes(x = reorder(term, mean_tfidf), y = mean_tfidf, fill = factor(cluster))) +
  geom_col() +
  coord_flip() +
  facet_wrap(~cluster, scales = "free_y") +
  labs(x = "Term", y = "Mean TF-IDF Weight", title = "Key Terms per Cluster")

print(cluster_comp)
```

```{r}
library(tidygraph)
library(ggraph)
library(ggforce)
library(dplyr)
library(tidyr)

# Get document-topic probabilities (gamma matrix)
doc_topics <- tidy(BoW_lda_model, matrix = "gamma")

# Calculate document-document similarity
doc_sim_matrix <- doc_topics %>%
  pivot_wider(names_from = topic, values_from = gamma) %>%
  column_to_rownames("document") %>%
  as.matrix()
  #z Transpose for doc-doc similarity
doc_sim_matrix <-   lsa::cosine(t(doc_sim_matrix))  # Cosine similarity
# Create document-document edges
tt_edges <- doc_sim_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column("from") %>%
  pivot_longer(-from, names_to = "to", values_to = "weight") %>%
  filter(from < to & weight > 0.2) %>%
  mutate(type = "doc-doc")

# Get top documents per topic
topic_docs <- doc_topics %>%
  group_by(topic) %>%
  slice_max(gamma, n = 100) %>%
  ungroup()

# Get top terms per topic
topic_terms <- tidy(BoW_lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup()

# Create document-term edges
dt_edges <- topic_docs %>%
  inner_join(topic_terms, by = "topic") %>%
  select(from = document, to = term, topic, weight = gamma) %>%
  mutate(type = "doc-term")

# Combine all edges
edges <- bind_rows(tt_edges, dt_edges) %>%
  mutate(weight = ifelse(is.na(weight), 1, weight))  # Default weight for doc-doc

# Create nodes data
nodes <- data.frame(
  name = unique(c(edges$from, edges$to)),
  type = ifelse(unique(c(edges$from, edges$to)) %in% topic_docs$document, 
                "document", "term")
) %>%
  left_join(
    bind_rows(
      doc_topics %>% 
        group_by(document) %>%
        slice_max(gamma, n = 1) %>%
        select(name = document, topic),
      topic_terms %>%
        group_by(term) %>%
        slice_max(beta, n = 1) %>%
        select(name = term, topic)
    ),
    by = "name"
  )

# Create graph
doc_term_graph <- tbl_graph(
  nodes = nodes,
  edges = edges,
  directed = TRUE
)

# Plot
ggraph(doc_term_graph, layout = "fr") + #, circular = TRUE) +
  # Document-term edges
  geom_edge_link(
    aes(filter = type == "doc-term", alpha = weight*0.1),
    color = "gray70",
    width = 0.3
  ) +
  # Document-document edges
  # geom_edge_link(
  #   aes(filter = type == "doc-doc", width = weight*0.01),
  #   color = "orange",
  #   alpha = 0.3
  # ) +
  # Nodes
  geom_node_point(
    aes(color = as.factor(topic), size = ifelse(type == "term", 5, 3)),
    alpha = 0.8
  ) +
  # Labels
  geom_node_text(
    aes(label = ifelse(type == "term", name, "")), 
    color = "black", size = 3, fontface = "bold", repel = TRUE
  ) +
  # Hulls
  geom_mark_hull(
    aes(x, y, group = topic, fill = as.factor(topic)),
    concavity = 1,
    expand = unit(2, "mm"),
    alpha = 0.05,
    size = 0.2
  ) +
  scale_edge_width(range = c(0.5, 2)) +
  labs(title = "Combined Document Network") +
  theme_void()
```

```{r}
library(dplyr)
library(tidyr)
library(tidygraph)
library(ggraph)

# Get dominant topic for each document
doc_topic_dominant <- doc_topics %>%
  group_by(document) %>%
  slice_max(gamma, n = 100) %>%
  ungroup()

# Get top terms per topic (for labeling)
topic_labels <- topic_terms %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  summarise(label = paste(term, collapse = ", "))

# Assign coordinates manually
nodes <- bind_rows(
  # Topics (inner circle)
  doc_topic_dominant %>%
    distinct(topic) %>%
    mutate(
      name = as.character(topic),
      type = "topic",
      angle = seq(0, 2 * pi, length.out = n() + 1)[-1],  # Evenly spaced
      x = cos(angle) * 1,  # Inner radius = 1
      y = sin(angle) * 1
    ),
  
  # Documents (outer circle)
  doc_topic_dominant %>%
    mutate(
      name = document,
      type = "document",
      angle = seq(0, 2 * pi, length.out = n() + 1)[-1],  # Evenly spaced
      x = cos(angle) * 2,  # Outer radius = 2
      y = sin(angle) * 2
    )
)

# Add topic labels
nodes <- nodes %>%
  left_join(topic_labels, by = "topic")

edges <- doc_topic_dominant %>%
  transmute(
    from = document,
    to = as.character(topic),
    weight = gamma
  )

graph <- tbl_graph(
  nodes = nodes,
  edges = edges,
  directed = FALSE
)

ggraph(graph, layout = "manual", x = x, y = y) +
  # Edges (document to topic)
  geom_edge_link(
    aes(alpha = weight),
    color = "gray50",
    show.legend = FALSE
  ) +
  # Topic nodes (inner circle)
  geom_node_point(
    aes(filter = type == "topic", color = as.factor(topic)),
    size = 10,
    alpha = 0.8
  ) +
  # Document nodes (outer circle)
  geom_node_point(
    aes(filter = type == "document"),
    size = 3,
    color = "steelblue",
    alpha = 0.6
  ) +
  # Topic labels (inner circle)
  geom_node_text(
    aes(filter = type == "topic", label = label),
    color = "black",
    size = 3,
    repel = FALSE  # Force exact positioning
  ) +
  # Document labels (outer circle)
  geom_node_text(
    aes(filter = type == "document", label = name),
    color = "black",
    size = 2,
    alpha = 0.7,
    repel = FALSE  # Force exact positioning
  ) +
  # Customize
  coord_fixed() +  # Ensure circles stay circular
  labs(title = "Topic-Document Network (Concentric Layout)") +
  theme_void()

```

# Lets try with using the go terms.
```{r}
function_df <- read.csv("../Data/Function_Descriptions.csv", row.names = 1)
# Check the exact gene name in UniProt first
function_descriptions <- function_df$GO

corpus <- Corpus(VectorSource(function_descriptions)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, paste0("\\b", stopwords("en"), "\\b")) %>%
  tm_map(stemDocument)

function_df$GO_corpus <- sapply(corpus, as.character)
function_df <- na.omit(function_df)


dtm <- function_df %>%
  unnest_tokens(word, GO_corpus) %>%
  count(Gene, word) %>%
  cast_dtm(Gene, word, n) 


# 1. Start with your original BoW_dtm (57x794)
#    Ensure it's a valid DocumentTermMatrix
stopifnot(inherits(dtm, "DocumentTermMatrix"))

# 2. Remove sparse terms FIRST (less aggressive threshold)
lda_dtm <- removeSparseTerms(dtm, sparse = 0.999)  # Start with 0.95, adjust as needed
cat("After term removal:", dim(lda_dtm), "\n")  # Should show [57 x reduced_terms]

# 3. Then remove empty documents
rowTotals <- rowSums(as.matrix(lda_dtm))
lda_dtm <- lda_dtm[rowTotals > 0, ]
cat("After doc removal:", dim(lda_dtm), "\n")  # e.g., [55 x 120]

# 4. Run LDA only if documents remain
if(nrow(lda_dtm) > 0) {
  lda_model <- LDA(lda_dtm, k = k, control = list(seed = 123))
} else {
  stop("All documents were filtered out! Adjust sparse term threshold.")
}
```
```{r}
top_terms <- tidy(lda_model, matrix = "beta") %>%  # From tidytext
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup()

ggplot(top_terms, aes(x = reorder(top_terms$term, top_terms$beta), y = beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  labs(title = "Top 10 Terms per Topic", x = "Term", y = "Beta (Probability)")

```
```{r}
```

```{r}
library(tidygraph)
library(ggraph)
library(ggforce)
library(dplyr)
library(tidyr)

# Get document-topic probabilities (gamma matrix)
doc_topics <- tidy(lda_model, matrix = "gamma")

# Calculate document-document similarity
doc_sim_matrix <- doc_topics %>%
  pivot_wider(names_from = topic, values_from = gamma) %>%
  column_to_rownames("document") %>%
  as.matrix()
  #z Transpose for doc-doc similarity
doc_sim_matrix <-   lsa::cosine(t(doc_sim_matrix))  # Cosine similarity
# Create document-document edges
tt_edges <- doc_sim_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column("from") %>%
  pivot_longer(-from, names_to = "to", values_to = "weight") %>%
  filter(from < to & weight > 0.2) %>%
  mutate(type = "doc-doc",
         topic = doc_topics$topic[match(from, doc_topics$document)])  # Add topic info)

# Get top documents per topic
topic_docs <- doc_topics %>%
  group_by(topic) %>%
  slice_max(gamma, n = 87) %>%
  ungroup()

# Get top terms per topic
topic_terms <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 1) %>%
  ungroup()

# Create document-term edges
dt_edges <- topic_docs %>%
  inner_join(topic_terms, by = "topic") %>%
  select(from = document, to = term, topic, weight = gamma) %>%
  mutate(type = "doc-term")

#Combine all edges
edges <- bind_rows(tt_edges, dt_edges) %>%
  
 mutate(weight = ifelse(is.na(weight), 1, weight))  # Default weight for doc-doc

# Create nodes data
nodes <- data.frame(
  name = unique(c(edges$from, edges$to)),
  type = ifelse(unique(c(edges$from, edges$to)) %in% topic_docs$document, 
                "document", "term")
) %>%
  left_join(
    bind_rows(
      doc_topics %>% 
        group_by(document) %>%
        slice_max(gamma, n = 1) %>%
        select(name = document, topic),
      topic_terms %>%
        group_by(term) %>%
        slice_max(beta, n = 1) %>%
        select(name = term, topic)
    ),
    by = "name"
  )

# Create graph
doc_term_graph <- tbl_graph(
  nodes = nodes,
  edges = edges,
  directed = TRUE
)

# Plot
ggraph(doc_term_graph, layout = "fr",) + #, circular = TRUE) +
  # Document-term edges
  geom_edge_arc2(
    aes(color = as.factor(topic), filter = type == "doc-term", alpha = weight),
    width = 0.3
  ) +
  # Document-document edges
  # geom_edge_link(
  #   aes(filter = type == "doc-doc", width = weight*0.01),
  #   color = "orange",
  #   alpha = 0.3
  # ) +
  # Nodes
  geom_node_point(
    aes(color = as.factor(topic), 
        size = ifelse(type == "term", 5, 3)),
    alpha = 0.8
  ) +
  # Labels
  geom_node_text(
    aes(label = name),  
    color = "black", size = 3, fontface = "bold", repel = TRUE
  ) +
  # Hulls
  # geom_mark_hull(
  #   aes(x, y, group = topic, fill = as.factor(topic)),
  #   concavity = 1,
  #   expand = unit(2, "mm"),
  #   alpha = 0.05,
  #   size = 0.2) +
  scale_edge_width(range = c(0.5, 2)) +
  labs(title = "Combined Document Network") +
  theme_void()
```

```{r}
library(clusterProfiler)

