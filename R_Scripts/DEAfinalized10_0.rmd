```{r}
# Load necessary libraries
library("tximport")
library("readr")
library("DESeq2")
library("tximportData")
library("ggplot2")
library("tximeta")
library("magrittr")
library("dplyr")
library("tidyverse")
library("ggpubr")
library("pheatmap")
library("grid")
library("ggplotify")
library("gridExtra")
library("vsn")
library("RColorBrewer")
library("PoiClaClu")
library("glmpca")
library("ggbeeswarm")
library("apeglm")
library("genefilter")
library("AnnotationHub")
library("tm") # Text mining
library("tidytext")
library("text2vec")
library("lsa")
library("topicmodels")
library("AnnotationDbi")
library("gridExtra")
library(tidygraph)
library(ggraph)
library(ggforce)
library(tidyr)
library(httr)
library(jsonlite)
library(pRoloc)
library(stringr)
library(viridis)

select <- function(...){ dplyr::select(...)}
```

```{R}
# List all of the quant.sf files
cwd <- getwd()
par_dir <- dirname(cwd)
quant_fp <- paste(par_dir, 'quants', sep = '/')
quant_files<- list.files(path = quant_fp, pattern = "quant.sf$", recursive = TRUE)
```

```{r}
# Extract sample names
quant_dirs <- list.files(quant_fp,
                        pattern = "_quant$",
                        full.names = TRUE)
sample_names <- gsub("_quant$", "", basename(quant_dirs))
# Correct for the FASTQ generation file included in the directory sample_names \<-
sample_names <- sample_names[1:48] # Ensure this range fits your data
```

```{r}
quant_fp
```

```{r}
# Create the txi import
txi <- tximport(files = paste(quant_fp, quant_files, sep = '/'), type = "salmon", txOut = TRUE)
```

```{r}
colnames(txi$abundance) <- sample_names
colnames(txi$counts) <- sample_names
colnames(txi$length) <- sample_names
```

```{r}
genotype <- c()
treatment <- c()
timepoint <- c()
replicate <- c()
for (i in 1:length(sample_names)){
  strippedConds = strsplit(sample_names, "\\_|\\-")
  genotype <- append(genotype, strippedConds[[i]][2])
  if (length(strippedConds[[i]]) == 4){
    treatment <- append(treatment, "Nind")
    if (strippedConds[[i]][3] == 1){
      timepoint <- append(timepoint, 5)
    } else if (strippedConds[[i]][3] == 3){
      timepoint <- append(timepoint, 18)
    }
    replicate <- append(replicate, strippedConds[[i]][4])
  }
  else if (length(strippedConds[[i]]) == 5){
    treatment = append(treatment, "ind")
    if (strippedConds[[i]][4] == 1){
      timepoint <- append(timepoint, 5)
    } else if (strippedConds[[i]][4] == 3){
      timepoint <- append(timepoint, 18)
    }
    replicate <- append(replicate, strippedConds[[i]][5])
  }

}
```

```{r}
samples <- sub("_L00",".",sample_names)
si_complete <-data.frame(samples=samples,
                         genotype=genotype,
                         treatment=treatment,
                         timepoint=timepoint,
                         replicate=replicate)
```
```{r}
si_complete
```
```{r}
coldata <- si_complete
coldata
coldata$names <- sub("_L00[0-9]+","",sample_names)
coldata$files <- paste(quant_fp, quant_files, sep="/")
coldata
```
```{r}
coldata <- subset(coldata, select = -replicate)
coldata
```

```{r}
se <- tximeta(coldata) #SummarizedExperiment Object
```
```{r}
if (!is.factor(se$treatment)){
  se$treatment <- factor(se$treatment)
}
if (!is.factor(se$timepoint)){
  se$timepoint <- factor(se$timepoint)
}
if (!is.factor(se$genotype)){
  se$genotype <- factor(se$genotype)
}
se$treatment %<>% relevel("Nind") # Nind is the base line to compare to.
se$timepoint %<>% relevel("5")
se$genotype %<>% relevel("WT")
```

```{r}
design =~ genotype*timepoint*treatment #~ genotype + timepoint + treatment + genotype:timepoint + genotype:treatment + genotype:timepoint:treatment

dds <- DESeqDataSet(se, design = design)
#dds <- collapseReplicates(dds, dds$samples, dds$replicate)
meanSdPlot(assay(dds), ranks = FALSE)

log.cts.one <- log2(assay(dds) + 1)
meanSdPlot(log.cts.one, ranks = FALSE)
```
# Run Prefiltering because DESeqDataSet contains several rows that contain zeros or small amounts. Prefiltering will remove these and allow for increased speed when running functions.
```{r}
nrow(dds)
smallestGroupSize <- 4
keep <- rowSums(counts(dds) >= 10) >= smallestGroupSize
dds <- dds[keep,]
nrow(dds)
```
# Lets look at the simulated data of what the counts should look like if they follow a poisson distribution.
```{r}
lambda <- 10^seq(from = -1, to = 2, length = 1000)
cts <- matrix(rpois(1000*100, lambda), ncol = 100)
meanSdPlot(cts, ranks = FALSE)

log.cts.one <- log2(cts + 1)
meanSdPlot(log.cts.one, ranks = FALSE)
```
# Now we will compare the variance stabilizing transformation (VST), regularized-logarithm transformation (rlog), and log2 transformation.
```{r}
vsd <- varianceStabilizingTransformation(dds, blind = FALSE)
#head(assay(vsd), 3)
```
```{r}
rld <- rlog(dds, blind = FALSE)
#head(assay(rld), 3)
```
```{r}
dds <- estimateSizeFactors(dds)

df <- bind_rows(
  as_data_frame(log2(counts(dds, normalized=TRUE)[, c(7,11)]+1)) %>%
         mutate(transformation = "log2(x + 1)"),
  as_data_frame(assay(vsd)[, c(7,11)]) %>% mutate(transformation = "vst"),
  as_data_frame(assay(rld)[, c(7,11)]) %>% mutate(transformation = "rlog"))

colnames(df)[1:2] <- c("x", "y")

lvls <- c("log2(x + 1)", "vst", "rlog")
df$transformation <- factor(df$transformation, levels=lvls)

ggplot(df, aes(x = x, y = y)) + geom_hex(bins = 80) +
  coord_fixed() + facet_grid( . ~ transformation)
```
# Now we can look at the euclidean distance between the sample to check for similarity between them.
```{r}
sampleDists <- dist(t(assay(dds)))
```
# And Create a heatmap to visualize.
```{r}
sampleDistMatrix <- as.matrix( sampleDists )
rownames(sampleDistMatrix) <- paste( dds$treatment, dds$genotype, dds$timepoint, sep = "-" )
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows = sampleDists,
         clustering_distance_cols = sampleDists,
         col = colors)
```
# Do the same using a poisson distance.
```{r}
poisd <- PoissonDistance(t(counts(dds)))
samplePoisDistMatrix <- as.matrix( poisd$dd )
rownames(samplePoisDistMatrix) <- paste( dds$treatment, dds$genotype, sep=" - " )
colnames(samplePoisDistMatrix) <- NULL
pheatmap(samplePoisDistMatrix,
         clustering_distance_rows = poisd$dd,
         clustering_distance_cols = poisd$dd,
         col = colors)
```
# Finally with a PCA.
```{r}
plotPCA(vsd, intgroup = c("treatment", "genotype", "timepoint"))
```
```{r}
pcaData <- plotPCA(vsd, intgroup = c( "treatment", "genotype", "timepoint"), returnData = TRUE)
pcaData
percentVar <- round(100 * attr(pcaData, "percentVar"))
ggplot(pcaData, aes(x = PC1, y = PC2, color = treatment, shape = genotype)) +
  geom_point(size =3, mapping = aes(alpha = timepoint)) +
  xlab(paste0("PC1: ", percentVar[1], "% variance")) +
  ylab(paste0("PC2: ", percentVar[2], "% variance")) +
  coord_fixed() +
  ggtitle("PCA with VST data")
```
# Finally, finally with a generalized PCA (GLM-PCA)
```{r}
gpca <- glmpca(counts(dds), L=2)
gpca.dat <- gpca$factors
gpca.dat$treatment <- dds$treatment
gpca.dat$genotype <- dds$genotype
gpca.dat$timepoint <- dds$timepoint

ggplot(gpca.dat, aes(x = dim1, y = dim2, color = treatment, shape = genotype)) + geom_point(size =3, mapping = aes(aplha = timepoint)) + coord_fixed() + ggtitle("glmpca - Generalized PCA")
```
# multidimensional scaling plot.
```{r}
mds <- as.data.frame(colData(vsd))  %>%
         cbind(cmdscale(sampleDistMatrix))
ggplot(mds, aes(x = `1`, y = `2`, color = treatment, shape = genotype, alpha = timepoint)) +
  geom_point(size = 3) + coord_fixed() + ggtitle("MDS with VST data")
```
```{r}
dds <- DESeq(dds)
res <- results(dds, name = "genotype_s4_vs_WT")
topGene <- rownames(res)[which.max(res$log2FoldChange)]
plotCounts(dds, gene = topGene, intgroup=c("treatment", "genotype", "timepoint"))
```

```{r}
geneCounts <- plotCounts(dds, gene = topGene, intgroup = c("treatment","genotype", "timepoint"),
                         returnData = TRUE)
ggplot(geneCounts, aes(x = timepoint, y = count, color = genotype, shape = treatment)) +
  scale_y_log10() +  geom_beeswarm(cex = 3)
```

```{r}
ggplot(geneCounts, aes(x = timepoint, y = count, color = genotype, group = interaction(genotype, treatment), shape = treatment)) +
  scale_y_log10() + geom_point(size = 3) + geom_line()
```


```{r, fig.height=20, fig.width=10}}
font = "Helvetica"
font_size = 12
timep = "5"
ntopgenes = 100
pthreshold = 0.1
design =~ genotype + treatment + timepoint + genotype:treatment + genotype:timepoint + treatment:timepoint + genotype:treatment:timepoint
#design =~ genotype + timepoint + treatment

dds <- DESeqDataSet(se, design = design)
dds <- DESeq(dds)
resultsNames(dds)

# Assuming 'rld' is your rlog-transformed data (or use vst()), filter
# for the top 50.
gene_variance <- apply(assay(rld), 1, var)  # Variance per gene

# Sort genes by variance (descending) and take top 50
top_genes <- names(sort(gene_variance, decreasing = TRUE)[1:50])
# Now create the dataframe for the heatmap

# Example: Extract and combine results for all 12 groups
heatmap_data_raw <- bind_rows(
  ###########################################################
  # Baseline effect with induction
  ###########################################################

  # Baseline (WT -treatment 5hr)
  results(dds, name = "Intercept") %>%
    as.data.frame() %>%
    rownames_to_column("Gene") %>%
    mutate(genotype = "WT", treatment = "ind", timepoint = timep, Comparison = "Baseline"
           ),

  ##########################################################################
  # Plasmid (non-functional) effect with and without induction and leakyniss
  ##########################################################################

  # NC -treatment 5hr (non-functional effector leakiness)
  results(dds, list(
  c("genotype_NC_vs_WT", "treatment_ind_vs_Nind",
    "genotypeNC.treatmentind", "timepoint_18_vs_5",
    "genotypeNC.timepoint18", "treatmentind.timepoint18",
    "genotypeNC.treatmentind.timepoint18")
    )) %>%
    as.data.frame() %>%
    rownames_to_column("Gene") %>%
    mutate(genotype = "NC", treatment = "ind", timepoint = timep, Comparison = "NC_IPTG_WT"
           ),

  ###########################################################
  # Functional effector with and without induction
  ###########################################################
    # s4 +treatment 18
  results(dds, contrast = list(
    c("genotype_s4_vs_WT", "treatment_ind_vs_Nind", "genotypes4.treatmentind", "timepoint_18_vs_5",
      "genotypes4.timepoint18", "treatmentind.timepoint18",
      "genotypes4.treatmentind.timepoint18"))) %>%
    as.data.frame() %>%
    rownames_to_column("Gene") %>%
    mutate(genotype = "s4", treatment = "ind", timepoint = timep, Comparison = "s4_IPTG_T2"
           ),

) %>%
  select(Gene, genotype, treatment, timepoint, log2FoldChange, padj) %>%
  mutate(
    sig_star = case_when(
      padj < 0.001 ~ "***",
      padj < 0.01 ~ "**",
      padj < 0.1 ~ "*",
      TRUE ~ ""
    ),
    # Force factor levels for plotting
    genotype = factor(genotype, levels = c("WT", "NC", "s4")),
    treatment = factor(treatment, levels = c("Nind", "ind")),
    timepoint = factor(timepoint, levels = c("5", "18"))
  )

heatmap_data <- heatmap_data_raw %>%
  dplyr::filter(
    !if_any(c(log2FoldChange, padj), is.na),  # Keep rows where these cols are non-NA
    padj < pthreshold  # Keep rows where these cols are non-zero
    )
write_csv(heatmap_data, "/Users/alec/Desktop/RNAanalysis/master_df.csv")
# Set symmetric color limits based on max absolute LFC
lfc_limit <- max(abs(heatmap_data$log2FoldChange), na.rm = TRUE) * 1.1
# Calculate optimal dimensions based on your data
n_genes <- length(unique(heatmap_data$Gene))
n_conditions <- length(unique(heatmap_data$GenoType))

# Set your target dimensions (in inches)
fig_width <- 10 # Your specified width
fig_height <- 50 # Your specified height

# Calculate optimal tile aspect ratio
tile_aspect_ratio <- (fig_height/fig_width)*(n_conditions/n_genes)

induction_order <- heatmap_data %>%
  # Filter to only the groups we want to compare
  filter(genotype %in% c("s4", "NC"), treatment == "ind", timepoint == timep) %>%
  # For each gene, calculate the difference between s4 and NC
  group_by(Gene) %>%
  summarize(
    induction_effect = log2FoldChange[genotype == "s4"] - log2FoldChange[genotype == "NC"]
  ) %>%
  # Sort by absolute induction effect
  arrange(desc(abs(induction_effect))) %>%
  pull(Gene)


rRNAgenes <- c("rrl*", "rrs*")
divergence_data <- heatmap_data %>%
  filter(
    (genotype == "WT" & treatment == "Nind" & timepoint == timep) |
    (genotype == "NC" & treatment == "ind" & timepoint == timep) |
    (genotype == "s4" & treatment == "ind" & timepoint == timep) |
    (!grepl(paste(rRNAgenes, collapse="|"), Gene, ignore.case = TRUE))
  ) %>%
  # Spread to wide format
  pivot_wider(
    id_cols = Gene,
    names_from = genotype,
    values_from = log2FoldChange,
  ) %>%
  # Calculate divergence metrics
  mutate(
    s4_vs_WT = `s4` - `WT`,  # Absolute difference from WT baseline
    NC_vs_WT = `NC` - `WT`,  # Absolute difference from NC control
    s4_vs_NC = `s4` - `NC`,  # Absolute difference from NC baseline
    combined_divergence = abs(s4_vs_WT - NC_vs_WT)  # Total divergence score
  ) %>%
  arrange(desc(s4_vs_NC)) # Sort by highest divergence

# Result contain:
# Gene | WT | NC | s4 | s4_vs_WT | s4_vs_NC | combined_divergence

# Sort data base of the divergence data order.
top_n_genes <- divergence_data %>%
  slice_head(n = ntopgenes) %>%  # Get first 20 rows (already sorted by descending divergence)
  pull(Gene)  # Extract as character vector

# Alternative with explicit sorting:
top_n_genes <- divergence_data %>%
  arrange(desc(combined_divergence)) %>%  # Ensure descending order
  head(ntopgenes) %>%
  pull(Gene)

gene_order <- top_n_genes

heatmap_data_sorted <- heatmap_data %>%
  filter(Gene %in% gene_order) %>%
  mutate(Gene = factor(Gene, levels = gene_order, exclude=NULL)) %>%
  arrange(Gene, .by_group = TRUE)

# 1. Calculate WT baseline values (per gene)
wt_baseline <- heatmap_data_sorted %>%
  filter(genotype == "WT",
        Gene %in% gene_order) %>%
  dplyr::select(Gene, treatment, timepoint, wt_l2fc = log2FoldChange)

nc_baseline <- heatmap_data_sorted %>%
  filter(genotype == "NC",
        Gene %in% gene_order) %>%
  dplyr::select(Gene, treatment, timepoint, nc_l2fc = log2FoldChange)

# 2. Create relative fold changes (s4-WT and NC-WT)
heatmap_relative_wt<- heatmap_data_sorted %>%
  filter(genotype %in% c("NC", "s4")) %>%  # Exclude WT
  inner_join(wt_baseline, by = c("Gene", "treatment", "timepoint")) %>%
  mutate(
    relative_l2fc = log2FoldChange - wt_l2fc,  # Calculate difference from WT
    genotype = paste0(genotype, "-WT"),  # Update genotype labels
    Gene = factor(Gene, levels = gene_order, exclude=NULL)  # Ensure consistent ordering)
  ) %>%
  select(-log2FoldChange, -wt_l2fc) %>%# Remove original columns
  dplyr::rename(`log2FoldChange` = `relative_l2fc`)  # Rename for consistency

# 3. (Optional) Combine with other annotations
heatmap_relative_wt<- heatmap_relative_wt%>%
  left_join(
    heatmap_data %>% select(Gene, genotype, sig_star),  # Keep significance stars
    by = c("Gene", "genotype")
  )

ggplot(heatmap_data_sorted, aes(
    x = interaction(genotype, treatment, timepoint, sep = " ", lex.order = TRUE),
    y = fct_rev(Gene))
) +
  theme_set(theme_grey(base_size = 12))+
  scale_y_discrete(limits = levels(rev(gene_order))) +
  geom_tile(aes(fill = log2FoldChange)) +
  #facet_grid(~ timepoint, scales = "free_x") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    #limits = c(-3, 3)  # Adjust based on your data
  ) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 8),
    axis.text.y = element_text(size = 6),
    strip.text = element_text(size = 10)
  ) +
  labs(
    x = "Strain + IPTG",
    y = "Gene",
    fill = expression(atop(log[2],"Fold Change"))
  )
```
# Now let's create a heatmap of the relative fold changes (subtracting the WT baseline).
```{r, fig.width=20, fig.height=10, font.size=12}

pad = 28
heatmap_relative_nc <- heatmap_data_sorted %>%
  filter(genotype == "s4") %>%  # Only NC
  inner_join(nc_baseline, by = c("Gene", "treatment", "timepoint")) %>%
  mutate(
    relative_l2fc = log2FoldChange - nc_l2fc,  # Calculate difference from NC
    genotype = paste0(genotype, "-NC"),  # Update genotype labels
    Gene = factor(Gene, levels = gene_order, exclude=NULL)  # Ensure consistent ordering
  ) %>%
  select(-log2FoldChange, -nc_l2fc) %>%# Remove original columns
  dplyr::rename(`log2FoldChange` = `relative_l2fc`)  # Sort by absolute log2FoldChange

heatmap_relative_nc <- heatmap_relative_nc %>%
  left_join(
    heatmap_data %>% select(Gene, genotype, sig_star),  # Keep significance stars
    by = c("Gene", "genotype")
  ) %>%
  arrange(abs(log2FoldChange), .by_group = TRUE)

heatmap_relative_nc$Gene <- factor(heatmap_relative_nc$Gene, levels = heatmap_relative_nc$Gene)
s4_vs_NC_heatmap <- ggplot(heatmap_relative_nc, aes(
    x = interaction(genotype, treatment, timepoint, sep = " ", lex.order = TRUE),
    y = fct_rev(Gene))
) +
  scale_y_discrete(limits = rev(top_n_genes)) +
  geom_tile(aes(fill = log2FoldChange, height= 0.9)) +
  #facet_grid(~ timepoint, scales = "free_x") +
  scale_fill_gradient2(
    low = "blue", mid = "white",  high = "red"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1, size = 8),
    strip.text = element_text(size = 10)
  ) +
  labs(
    x = "Strain + IPTG",
    y = "Gene",
    fill = expression(atop(log[2],"Fold Change"))
  )

s4_vs_NC_barchart <- ggplot(heatmap_relative_nc, aes(
    x = Gene,
    y = log2FoldChange
  )) +
  geom_bar(stat = "identity") +
  scale_x_discrete(limits = rev) +
  theme(axis.text.x = element_text(angle=90, family=font, size=fsize+10),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  scale_fill_gradient2(
    low = "blue", mid = "grey", high = "red",
    limits = c(-20, 20)) +  # Adjust based on your data ) +
  labs(
    x = "Gene",
    y = expression(log[2]~" Fold Change"),
    #title = "Relative Fold Changes (s4 vs NC)"
  )

s4_vs_NC_histogram <- ggplot(heatmap_relative_nc, aes(x = log2FoldChange)) +
  geom_histogram(bins = 25, fill = "blue", color = "black", alpha = 0.7) +
  labs(
    x = expression(log[2]~" Fold Change"),
    y = "Frequency",
    #title = "Distribution of Relative Fold Changes (s4 vs NC)"
  ) + 
  theme(plot.margin = margin(t=pad, b=pad, l=pad, r=pad))

lay <- rbind(c(1, 2, 2),
             c(1, 2, 2),
             c(1, 3, 3))
grid <- grid.arrange(
  s4_vs_NC_heatmap,
  s4_vs_NC_barchart,
  s4_vs_NC_histogram,
  ncol = 2,
  layout_matrix = lay 
)

ggsave("/Users/alec/Documents/GradSchool/Yeung_Lab/GroupMeetings/GroupMeetingPresentation2025_05_16 /Slides/Figures/Heatmap_5hr.png",
      p,
      width = 5,
      height = 10,
      dpi = 300,)


```
```{r}

```


```{r}
positive_count <- heatmap_relative_nc %>%
  filter(log2FoldChange > 0) %>%
  nrow()

negative_count <- heatmap_relative_nc %>%
  filter(log2FoldChange < 0) %>%
  nrow()

print(paste0("Number of genes with positive log2FoldChange: ", positive_count, "Number of genes with negative log2FoldChange: ", negative_count))
```


```{r}
genes <- c(
  # SOS responsive, LexA-dependent genes
  "polB", "recA", "recN", "sbmC", "ssb", "sulA", "uvrA", "uvrB",

  # LexA-independent genes
  "dnaA", "nrdA", "nrdB", "cydA", "tdcB", "tdcC", "cstA", "sspA", "sspB",
  "deoA", "deoB", "deoC", "dnaG", "gmk", "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG",

  # Anaerobic induction
  "cydA", "tdcB", "tdcC",

  # Starvation induction
  "cstA", "sspA", "sspB",

  # Other regulation
  "deoA", "deoB", "deoC", "dnaG", "gmk", "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG",

  # Transcription-related genes
  "fliA", "fliZ", "greA", "rho", "rpoA", "rpoD",

  # Translation-related genes
  "fusA", "infA", "rnpA", "rpmB", "rpmG", "rpmH", "rpsG", "rpsU", "trmD",

  # Hypothetical or unknown function genes
  "yabO", "yagP", "ychB", "ydiY", "yebE", "yebF", "yeeN", "yfgB", "yfhN",
  "yfhO", "yghB", "yhjG", "yi52–10", "yibB", "yjeS", "ylaC", "yleA"
)

gene_functions <- data.frame(
  
)

# Create a data frame for norfloxacin expression values from Table 1
nor_df <- data.frame(
  Gene = c("polB", "recA", "recN", "sbmC", "ssb", "sulA", "uvrA", "uvrB", "dnaA", "nrdA", "nrdB",
           "fliA", "fliZ", "greA", "rho", "rpoA", "rpoD", "deoA", "deoB", "deoC", "dnaG", "gmk",
           "gyrA", "gyrB", "intZ", "mcrB", "oraA", "pyrG", "yabO", "yagP", "ychB", "ydiY", "yebE",
           "yfgB", "yfhN", "yfhO", "yghB", "yhjG", "yi52–10", "yibB", "yjeS", "ylaC", "yleA"),

  Nor_0.5_MIC = c(0.93, 3.79, 2.36, 1.14, 0.85, 1.36, 1.49, 1.25, 0.95, 1.48, 1.27, 1.18, 0.69,
                  0.99, 1.04, 0.86, 0.99, 1.12, 1.21, 1.19, 0.98, 1.03, 0.97, 0.76, 0.84, 1.02,
                  0.98, 1.04, 0.93, 0.97, 0.86, 0.86, 1.09, 0.97, 0.93, 0.83, 0.98, 0.87, 1.02,
                  1.00, 0.96, 0.85, 0.94),

  Nor_1_MIC = c(1.22, 6.37, 3.65, 2.78, 1.06, 2.84, 2.12, 1.23, 1.25, 1.30, 1.08, 0.88, 0.71,
                0.97, 1.02, 0.80, 2.05, 1.28, 1.29, 1.15, 1.11, 1.06, 0.88, 1.00, 0.85, 0.92,
                0.92, 0.94, 0.93, 0.86, 0.97, 0.88, 0.97, 0.89, 0.89, 0.75, 0.91, 0.69, 0.86,
                0.94, 0.81, 0.92, 0.81),

  Nor_2_MIC = c(1.31, 11.10, 4.82, 5.59, 1.00, 4.50, 1.97, 1.39, 1.16, 1.65, 1.48, 0.98, 0.39,
                1.93, 0.67, 0.75, 4.64, 1.32, 3.25, 1.42, 1.69, 2.35, 0.84, 0.97, 0.81, 1.03,
                1.21, 1.03, 1.07, 1.03, 1.62, 0.92, 1.01, 1.62, 1.27, 1.74, 1.08, 0.94, 1.03,
                1.53, 1.21, 2.39, 1.43),

  Nor_33_MIC = c(4.84, 47.61, 16.14, 0.93, 2.94, 6.44, 21.63, 3.40, 3.65, 8.92, 3.96, 2.81,
                 2.72, 4.35, 3.23, 0.35, 7.85, 2.25, 4.09, 5.24, 4.69, 2.69, 7.07, 5.09, 2.64,
                 4.54, 5.11, 7.41, 3.71, 2.83, 5.21, 3.46, 1.52, 2.09, 3.18, 4.44, 4.08, 2.90,
                 2.35, 3.18, 2.78, 3.18, 3.56),

  Nor_133_MIC = c(4.27, 47.55, 11.69, 0.83, 3.75, 4.98, 46.35, 4.71, 3.68, 5.17, 2.07, 4.44,
                  5.25, 3.86, 4.09, 0.22, 9.45, 1.75, 6.07, 6.83, 5.73, 3.95, 8.85, 6.02, 3.64,
                  5.05, 5.35, 9.76, 3.61, 3.56, 3.63, 2.44, 1.66, 3.01, 3.26, 5.28, 6.82, 4.93,
                  3.43, 4.68, 3.98, 6.25, 6.25)
)

# View the data frame
rownames(nor_df)<-nor_df$Gene
nor_df <- log(nor_df[,-1], 2)
```

```{r}
heatmap_data_comp <- heatmap_data %>%
  filter(genotype == "s4", treatment == "ind", timepoint == timep) %>%
  select(Gene, log2FoldChange) %>%
  remove_rownames() %>%
  column_to_rownames(var="Gene")

gene_overlap <- rownames(nor_df) %in% rownames(heatmap_data_comp)
subset_heatmap <- heatmap_data_comp[rownames(heatmap_data_comp) %in% rownames(nor_df), , drop=FALSE]
subset_nor_df <- nor_df[rownames(nor_df) %in% rownames(heatmap_data_comp), , drop=FALSE]

subset_df <- Reduce(function(x,y) merge(x,y, by='row.names', all=T), list(subset_heatmap, subset_nor_df))
rownames(subset_df)<-subset_df$Row.names
subset_df <- subset_df[, -1]
```

```{r, fig.width=10, fig.height=5, font.size=12}
var_nor <- apply(nor_df, MARGIN=1, FUN= var)
var_nor_df = data.frame(Var=var_nor)
rownames(var_nor_df) <- rownames(nor_df)

varNor_l2f_df <- nor_df[order(var_nor_df$Var, decreasing=FALSE), , drop=FALSE]
varNor_l2f_df

varNor_l2f_df <- varNor_l2f_df %>% rownames_to_column() %>% gather(colname, value, -rowname)

varNor_l2f_df <- varNor_l2f_df %>% mutate(colname = case_when(
  colname == "Nor_0.5_MIC" ~ "0.5",
  colname == "Nor_1_MIC" ~ "1",
  colname == "Nor_2_MIC" ~ "2",
  colname == "Nor_33_MIC" ~ "33",
  colname == "Nor_133_MIC" ~ "133"
))

columns <- c("Norfloxacin\n0.5MIC", "Norfloxacin\n1MIC", "Norfloxacin\n2MIC", "Norfloxacin\n33MIC", "Norfloxacin\n133MIC")
varNor_l2f_df$colname <- factor(varNor_l2f_df$colname, levels=unique(varNor_l2f_df$colname))
varNor_l2f_df
varNor_l2f_df$rowname<-factor(varNor_l2f_df$rowname, levels=unique(varNor_l2f_df$rowname))

fsize = 10

nor_top_hits <- ggplot(varNor_l2f_df, aes(x=colname, y=rowname, fill=value)) +
  geom_tile() +
  #scale_fill_distiller(palette="RdGy") +
  theme(axis.text.x = element_text(angle=0, family=font, size=fsize+2)) +
  theme(axis.text.y = element_text(family=font, size=fsize-2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  coord_cartesian(clip = "off") +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred",
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  #scale_x_discrete(labels = columns) +
  xlab("Norfloxacin [MIC]") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change")),
       #title = "Relative Fold Changes (Norfloxacin)")
  )
print(nor_top_hits)
ggsave("/Users/alec/Documents/GradSchool/Yeung_Lab/GroupMeetings/GroupMeetingPresentation2025_05_16 /Slides/Figures//NorTopHits.png", nor_top_hits, dpi=600)
```

```{r, fig.width=10, fig.height=5, font.size=12}
x_ann1 <- textGrob("timepointpoint: 5hr.", gp=gpar(fontsize=10))
x_ann2 <- textGrob("timepointpoint: 18hr.", gp=gpar(fontsize=10))

subset_sum_df<- data.frame(sort(rowSums(abs(subset_df)), decreasing=FALSE))
subset_df <- subset_df[rownames(subset_sum_df),]

subset_df2 <- subset_df %>% rownames_to_column() %>% gather(colname, value, -rowname)

subset_df2 <- subset_df2 %>% mutate(group = case_when(
  grepl("log2FoldChange", colname)~"dCasRx",
  grepl("Nor", colname)~"Norfloxacin"
))
subset_df2 <- subset_df2 %>% mutate(label = case_when(
  colname == "log2FoldChange" ~ "dCasRx\n5hr",
  colname == "Nor_0.5_MIC" ~ "Norfloxacin\n0.5MIC",
  colname == "Nor_1_MIC" ~ "Norfloxacin\n1MIC",
  colname == "Nor_2_MIC" ~ "Norfloxacin\n2MIC",
  colname == "Nor_33_MIC" ~ "Norfloxacin\n33MIC",
  colname == "Nor_133_MIC" ~ "Norfloxacin\n133MIC"
))

subset_df2$label <- factor(subset_df2$label, levels=c("dCasRx\n5hr", "Norfloxacin\n0.5MIC", "Norfloxacin\n1MIC", "Norfloxacin\n2MIC", "Norfloxacin\n33MIC", "Norfloxacin\n133MIC"))
subset_df2$colname  <- factor(subset_df2$colname, levels=c("Targeting5hr", "Nor_0.5_MIC", "Nor_1_MIC", "Nor_2_MIC", "Nor_33_MIC", "Nor_133_MIC"))
subset_df2$rowname <- factor(subset_df2$rowname, levels=unique(subset_df2$rowname))

CasRx_df <- subset_df2 %>% filter(group=="CasRx")
Nor_df <- subset_df2 %>% filter(group=="Norfloxacin")

trt_comp <- ggplot(subset_df2, aes(x=label, y=rowname, fill=value)) +
  geom_tile() +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  theme(axis.text.x = element_text(angle=0, hjust = 1, family=font),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  #theme( plot.margin=unit(c(2,2,2,2), "lines")) +
  #annotation_custom(x_ann1, xmin=1.5, xmax=1, ymin=6, ymax=6) +
  #annotation_custom(x_ann2, xmin= 3.5, xmax=3.5, ymin=6, ymax=6)+
  #facet_wrap( ~group, scales="free_x" ) +
  #scale_x_discrete(labels = c("Non-Targeting gRNA", "Non-Targeting gRNA", "Targeing gRNA", "Targeting gRNA", "Norfloxacin 0.5MIC", "Norfloxacin 1MIC", "Norfloxacin 2MIC", "Norfloxacin 33MIC", "Norfloxacin 133MIC")) +
  coord_cartesian(clip = "off") +
  xlab("Treatment") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change")),
       #title = "Comparison of Relative Fold Changes (dCasRx vs Norfloxacin)"
  )
print(trt_comp)
ggsave("/Users/alec/Documents/GradSchool/Yeung_Lab/GroupMeetings/GroupMeetingPresentation2025_05_16 /Slides/Figures/trt_compNC_vs_s4.png", trt_comp, dpi=600)
```
```{r, fig.width=20, fig.height=10, font.size=12}
fsize <- 16
pad = 28
# -------------------------------------------------------------------------------
# barchart of relative foldchanges.
# -------------------------------------------------------------------------------
s4_vs_NC_barchart <- ggplot(heatmap_relative_nc, aes(
    x = Gene,
    y = log2FoldChange
  )) +
  geom_bar(stat = "identity") +
  scale_x_discrete(limits = rev) +
  theme(axis.text.x = element_text(angle=90, family=font, size=fsize),
        axis.text.y = element_text(angle=0, family=font, size=fsize),
        axis.title = element_text(family=font, size=fsize+2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  scale_fill_gradient2(
    low = "blue", mid = "grey", high = "red",
    limits = c(-20, 20)) +  # Adjust based on your data ) +
  labs(
    x = "Gene",
    y = expression(log[2]~" Fold Change")
    #title = "Relative Fold Changes (s4 vs NC)"
  )

# -------------------------------------------------------------------------------
# Histogram of relative fold changes
# -------------------------------------------------------------------------------
s4_vs_NC_histogram <- ggplot(heatmap_relative_nc, aes(x = log2FoldChange)) +
  geom_histogram(bins = 25, fill = "blue", color = "black", alpha = 0.7) +
  theme(axis.text.x = element_text(angle=0, family=font, size=fsize),
        axis.text.y = element_text(angle=0, family=font, size=fsize),
        axis.title = element_text(family=font, size=fsize+2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  labs(
    x = expression(log[2]~" Fold Change"),
    y = "Frequency",
    #title = "Distribution of Relative Fold Changes (s4 vs NC)"
  ) + 
  theme(plot.margin = margin(t=pad, b=pad, l=pad, r=pad)
)
# -------------------------------------------------------------------------------
# Heatmap of Norfloxacin expression values
# -------------------------------------------------------------------------------
nor_top_hits <- ggplot(varNor_l2f_df, aes(x=colname, y=rowname, fill=value)) +
  geom_tile() +
  #scale_fill_distiller(palette="RdGy") +
  theme(axis.text.x = element_text(angle=0, family=font, size=fsize),
        axis.text.y = element_text(angle=0, family=font, size=fsize-2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad-5),
        axis.title = element_text(family=font, size=fsize+2)) +
  coord_cartesian(clip = "off") +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred",
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  #scale_x_discrete(labels = columns) +
  xlab("Norfloxacin [MIC]") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change")),
       #title = "Relative Fold Changes (Norfloxacin)")
  )
# -------------------------------------------------------------------------------
# Heatmap of treatment comparisons
# -------------------------------------------------------------------------------
trt_comp <- ggplot(subset_df2, aes(x=label, y=rowname, fill=value)) +
  geom_tile() +
  scale_fill_gradient2(
    low = "darkblue",  high = "darkred"
    #limits = c(-20, 20)  # Adjust based on your data
  ) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, family=font, size=fsize),
        axis.text.y = element_text(angle=0, vjust = 0, family=font, size=fsize),
        axis.title = element_text(family=font, size=fsize+2),
        plot.margin = margin(t=pad, b=pad, l=pad-10, r=pad)) +
  #theme( plot.margin=unit(c(2,2,2,2), "lines")) +
  #annotation_custom(x_ann1, xmin=1.5, xmax=1, ymin=6, ymax=6) +
  #annotation_custom(x_ann2, xmin= 3.5, xmax=3.5, ymin=6, ymax=6)+
  #facet_wrap( ~group, scales="free_x" ) +
  #scale_x_discrete(labels = c("Non-Targeting gRNA", "Non-Targeting gRNA", "Targeing gRNA", "Targeting gRNA", "Norfloxacin 0.5MIC", "Norfloxacin 1MIC", "Norfloxacin 2MIC", "Norfloxacin 33MIC", "Norfloxacin 133MIC")) +
  coord_cartesian(clip = "off") +
  xlab("Treatment") +
  ylab("Gene")+
  labs(fill=expression(atop(log[2],"Fold Change")),
       #title = "Comparison of Relative Fold Changes (dCasRx vs Norfloxacin)"
  )

lay <- rbind(c(1, 1, 1),
             c(2, 3, 4),
             c(2, 3, 4))

grid <- grid.arrange(
  s4_vs_NC_barchart,
  s4_vs_NC_histogram,
  nor_top_hits,
  trt_comp,
  ncol = 2,
  layout_matrix = lay,
  padding = 5
)

ggsave("../Figures/Figure5.png",
       grid,
       width = 20,
       height = 10,
       dpi = 300,
       units = "in")


```
```{r}
# Arrange master df in order or decreasing absolute log2FoldChange for only s4
sorted_master_df <- Master_df %>%
  arrange(desc(abs(log2FoldChange))) 


```


```{r, fig.width=20, fig.height=10, font.size=12}
fsize <- 16
pad = 28
# -------------------------------------------------------------------------------
# barchart of relative foldchanges.
# -------------------------------------------------------------------------------
s4_vs_NC_barchart <- ggplot(heatmap_relative_nc, aes(
    x = Gene,
    y = log2FoldChange
  )) +
  geom_bar(stat = "identity") +
  scale_x_discrete(limits = rev) +
  theme(axis.text.x = element_text(angle=90, family=font, size=fsize),
        axis.text.y = element_text(angle=0, family=font, size=fsize),
        axis.title = element_text(family=font, size=fsize+2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  scale_fill_gradient2(
    low = "blue", mid = "grey", high = "red",
    limits = c(-20, 20)) +  # Adjust based on your data ) +
  labs(
    x = "Gene",
    y = expression(log[2]~" Fold Change")
    #title = "Relative Fold Changes (s4 vs NC)"
  )

# -------------------------------------------------------------------------------
# Histogram of relative fold changes
# -------------------------------------------------------------------------------
s4_vs_NC_histogram <- ggplot(heatmap_relative_nc, aes(x = log2FoldChange)) +
  geom_histogram(bins = 25, fill = "blue", color = "black", alpha = 0.7) +
  theme(axis.text.x = element_text(angle=0, family=font, size=fsize),
        axis.text.y = element_text(angle=0, family=font, size=fsize),
        axis.title = element_text(family=font, size=fsize+2),
        plot.margin = margin(t=pad, b=pad, l=pad, r=pad)) +
  labs(
    x = expression(log[2]~" Fold Change"),
    y = "Frequency",
    #title = "Distribution of Relative Fold Changes (s4 vs NC)"
  ) + 
  theme(plot.margin = margin(t=pad, b=pad, l=pad, r=pad)
)

empty_plot <- ggplot() +
  theme_void() +
  theme(plot.margin = margin(t=pad, b=pad, l=pad, r=pad))

lay <- rbind(c(1, 1),
             c(2, 3))

grid <- grid.arrange(
  s4_vs_NC_barchart,
  s4_vs_NC_histogram,
  empty_plot,
  ncol = 2,
  layout_matrix = lay,
  padding = 5
)

ggsave("/Users/alec/Documents/GradSchool/Yeung_Lab/T32 Symposium Slides/PNGs/Figure5B_dCasRxBarChart_and_hist.png",
       grid,
       width = 30,
       height = 15,
       dpi = 300,
       units = "in")
```

# Pull the gene functions from Uniprot.
```{r}
get_go_terms <- function(entry) {
  if (!is.null(entry$dbReferences) &&
      "type" %in% names(entry$dbReferences)) {
    go_terms <- entry$dbReferences[entry$dbReferences$type == "GO", ]
    if (nrow(go_terms) > 0) {
      return(go_terms[, c("id", "properties.term")])
    }
  }
  return(data.frame(id=character(), term=character())) # Empty df if no GO terms
}

library(UniProt.ws)
up <- UniProt.ws(taxId = 83333)  # Automatically loads latest UniProt data

# Fetch GO terms for TP53, BRCA1, EGFR
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
library(org.EcK12.eg.db)

# 1. Get ALL UniProt IDs for your genes (including Trembl)
genes <- as.character(heatmap_data_sorted$Gene)
all_genes <- data.frame(Gene = heatmap_data_sorted$Gene) %>%
  mutate(ENTREZID = mapIds(
    org.EcK12.eg.db,
    keys = genes,
    keytype = "SYMBOL",
    column = "ENTREZID",
    multiVals = "first"
  ))

go_df <- AnnotationDbi::select(up,
                          keys = all_genes$ENTREZID,
                          columns = c("go"),
                          keytype = "GeneID"
                          )

function_df <- all_genes %>%
  inner_join(go_df, by = c("ENTREZID" = "From")) %>%
  dplyr::rename(UniProtID = `Entry`, Functions = colnames(.)[[4]]) %>%
  # Split into individual term-GO pairs
  mutate(pairs = str_split(Functions, ";\\s*")) %>%
  unnest(pairs) %>%
  filter(pairs != "") %>%
  # Extract clean components
  mutate(
    function_term = str_replace_all(pairs, "\\s*\\[[^]]+\\]", "") %>% str_trim(),
    go_id = str_extract(pairs, "GO:\\d+")
  ) %>%
  # Group by gene and combine terms
  group_by(Gene, UniProtID, ENTREZID) %>%
  summarize(
    Functions = paste(unique(function_term[function_term != ""]), collapse = ", "),
    GO = paste(unique(go_id[!is.na(go_id)]), collapse = ", "),
    .groups = "drop"
  ) %>%
  # Remove any rows with empty GO terms
  filter(GO != "")
write.csv(heatmap_data_sorted, file = "../Data/heatmap_data.csv", row.names = TRUE)
write.csv(function_df, file = "../Data/Function_Descriptions.csv", row.names = TRUE)
# Now let's analyze the functions and cluster using natural language process approaches.
```
# Create corpus based of function.
```{r}
function_df <- read.csv("../Data/Function_Descriptions.csv", row.names = 1)
# Check the exact gene name in UniProt first
function_descriptions <- function_df$Functions

corpus <- Corpus(VectorSource(function_descriptions)) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, paste0("\\b", stopwords("en"), "\\b"))
  #tm_map(stemDocument)

function_df$Functions <- sapply(corpus, as.character)
#function_df <- na.omit(function_df)
#function_df <- subset(function_df, select = - function_term)
Master_df <- inner_join(function_df, heatmap_data, by = "Gene")
```
# search for PPP and Pnt Genes.
```{r}
ppp_go_ids <- c("GO:0006098", "GO:0019321", "GO:0055114")  
pntab_go_id <- "GO:0008750"  

ppp_go_ids <- c("GO:0006098", "GO:0019321", "GO:0055114")

ppp_genes <- Master_df %>%
  filter(str_detect(GO, paste(ppp_go_ids, collapse = "|"))) %>%
  select(Gene, Functions, GO, log2FoldChange, sig_star)

pntab_go_id <- "GO:0008750"

pntab_genes <- Master_df %>%
  filter(str_detect(GO, pntab_go_id)) %>%
  select(Gene, Functions, GO, log2FoldChange, sig_star)
```



# Vectorize the corpus using A Bag-of-Words with TF-IDF weighting and Word Embeddings via Glove or Word2Vec.
```{r}
# A Bag-of-Words with TF-IDF weighting.
k<-3

BoW_dtm <- function_df %>%
  unnest_tokens(word, Functions) %>%
  dplyr::count(Gene, word) %>%
  cast_dtm(Gene, word, n)

library(tm)
library(topicmodels)

# 1. Start with your original BoW_dtm (57x794)
#    Ensure it's a valid DocumentTermMatrix
stopifnot(inherits(BoW_dtm, "DocumentTermMatrix"))

# 2. Remove sparse terms FIRST (less aggressive threshold)
BoW_lda_dtm <- removeSparseTerms(BoW_dtm, sparse = 0.999)  # Start with 0.95, adjust as needed
cat("After term removal:", dim(BoW_lda_dtm), "\n")  # Should show [57 x reduced_terms]

# 3. Then remove empty documents
BoW_rowTotals <- rowSums(as.matrix(BoW_lda_dtm))
BoW_lda_dtm <- BoW_lda_dtm[BoW_rowTotals > 0, ]
cat("After doc removal:", dim(BoW_lda_dtm), "\n")  # e.g., [55 x 120]

# 4. Run LDA only if documents remain
if(nrow(BoW_lda_dtm) > 0) {
  BoW_lda_model <- LDA(BoW_lda_dtm, k = k, control = list(seed = 123))
} else {
  stop("All documents were filtered out! Adjust sparse term threshold.")
}

it <- itoken(function_df$Functions,
             ids = function_df$Gene,
             tokenizer = word_tokenizer,
             progressbar = TRUE)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
t2v_dtm <- create_dtm(it, vectorizer)

rownames(BoW_dtm) <- function_df$Gene[1:nrow(BoW_dtm)]

# 4. Verify
print(class(t2v_dtm))  # Should show "dgCMatrix" "dsparseMatrix" "generalMatrix"
print(dim(t2v_dtm))    # Should show [documents x terms]

rownames(t2v_dtm) <- function_df$Gene[1:nrow(t2v_dtm)]
colnames(t2v_dtm) <- vocab$term

# 1. Verify input data
cat("\n=== Input Data ===\n")
print(head(function_df$function_term))
print(length(function_df$function_term))

# 2. Check iterator
it <- itoken(function_df$Functions)
cat("\n=== Iterator ===\n")
print(class(it))

# 3. Check vocabulary
vocab <- create_vocabulary(it)
cat("\n=== Vocabulary ===\n")
print(dim(vocab))

# 4. Create and inspect DTM
t2v_dtm <- create_dtm(it, vocab_vectorizer(vocab))
cat("\n=== Final DTM ===\n")
print(class(t2v_dtm))
print(dim(t2v_dtm))
print(object.size(t2v_dtm))

t2v_dtm <- t2v_dtm[rowSums(t2v_dtm) > 0, ] # Remove empty rows
```

```{r}
empty_rows <- which(rowSums(as.matrix(BoW_dtm)) == 0)
function_df[empty_rows, c("Gene", "ENTREZID")]
```

# Try different clustering methods.
```{r}
# k-means clustering
BoW_kmeans_res <- kmeans(as.matrix(BoW_dtm), centers = k)
#t2v_kmeans_res <- kmeans(as.matrix(t2v_dtm), centers = k)
function_df$BoW_k_means_cluster <- as.factor(BoW_kmeans_res$cluster)
#function_df$t2v_k_means_cluster <- as.factor(t2v_kmeans_res$cluster)

# Hierarchical clustering
# Use the cosine similarity which is ideal for text data.
BoW_cosine_matrix <- as.matrix(BoW_dtm)
BoW_cosine_sim <- lsa::cosine(t(BoW_cosine_matrix))
BoW_cosine_dist <- as.dist(1 - BoW_cosine_sim)
BoW_hclust <- hclust(BoW_cosine_dist, method = "ward.D2")


t2v_lda_dtm <- DocumentTermMatrix(t2v_dtm, control = list(weighting = weightTf))
t2v_lda_dtm <- removeSparseTerms(t2v_lda_dtm, 0.9999)
t2v_rowTotals <- apply(t2v_lda_dtm, 1, sum)
t2v_lda_dtm <- t2v_lda_dtm[t2v_rowTotals > 0, ]
t2v_lda_model <- LDA(t2v_lda_dtm, k = k, control = list(seed = 123))

# Note that the above commented out code does not work because of the way text2vec stores it's dtm.
# compared to the tm library, t2v creates a raw term-count matrix, lacking explicit storage of wieghting attributes,
# using simple counts that are equivelent to tm:weightTf, and using a more memory efficient sparse matrix format.
# To fix this we could either covert to a tm DTM or use text2vec directly. Additionally we could add a weigting attribute
# to the t2v dtm using
#    attr(t2v_dtm, "weighting") <- c("term frequency", "tf")
#    class(t2v_dtm) <- c("DocumentTermMatrix", "simple_triplet_matrix")
# We'll use t2v directly for now.
cat("LDA documents:", nrow(BoW_lda_model@gamma), "\n")  # Should match:
cat("DTM documents:", nrow(BoW_lda_dtm), "\n")          # Should match:
cat("Dataframe rows:", nrow(function_df), "\n")          # The mismatch

# Now do verification
Matrix::nnzero(t2v_lda_dtm)

# Inspect first document

# 1. Check matrix type
print(class(t2v_dtm))  # Should show "dgCMatrix"

# 2. Inspect first non-empty document
# 1. Find first non-empty document
nonzero_docs <- which(rowSums(t2v_dtm) > 0)[1]
print(paste("First non-empty gene at position:", nonzero_docs))

# Complete diagnostics.
# 1. Check current object
cat("Object type:", class(t2v_dtm), "\n")
cat("Dimensions:", if(is.null(dim(t2v_dtm))) "vector" else dim(t2v_dtm), "\n")

# 2. If corrupted, recreate
if(!inherits(t2v_dtm, "dgCMatrix")) {
  warning("Recreating DTM...")
  t2v_dtm <- create_dtm(itoken(your_text_data),
                     vocab_vectorizer(create_vocabulary(itoken(your_text_data))))
}

# 3. Safe inspection
if(nrow(t2v_dtm) > 0) {
  doc_index <- which(rowSums(t2v_dtm) > 0)[1]
  sparse_slice <- t2v_dtm[doc_index, , drop = FALSE]

  cat("\nFirst non-empty doc:", rownames(t2v_dtm)[doc_index], "\n")
  print(data.frame(
    term = colnames(t2v_dtm)[sparse_slice@i + 1],
    count = sparse_slice@x
  ))
} else {
  warning("No documents with terms found!")
}

doc_index <- which(rowSums(t2v_dtm) > 0)[1]
```
# Now visualize
```{r}
# Extract top 10 terms per topic
BoW_top_terms <- tidy(BoW_lda_model, matrix = "beta") %>%  # From tidytext
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup()

t2v_top_terms <- tidy(t2v_lda_model, matrix = "beta") %>%  # From tidytext
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup()

# Plot
plots <- list()
plots[[1]] <- ggplot(BoW_top_terms, aes(x = reorder(BoW_top_terms$term, BoW_top_terms$beta), y = beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  labs(title = "Bag of Words\nTop 10 Terms per Topic", x = "Term", y = "Beta (Probability)")

plots[[2]] <- ggplot(t2v_top_terms, aes(x = reorder(t2v_top_terms$term, beta), y = beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  coord_flip() +
  labs(title = "text2vec\nTop 10 Terms per Topic", x = "Term", y = "Beta (Probability)")


library(patchwork)
combined_plot <- wrap_plots(plots, ncol = 2) +
  plot_annotation(title = "Comparison of Gene Sorting Methods") &
  theme_minimal()
print(combined_plot)
```
# Dertermine the top contributing tokens for each cluster.
```{r, fig.align="center, echo= FALSE, fig.width=5, fig.height=20}
# Assuming:
# - `clusters` is a vector of cluster assignments (e.g., from kmeans$cluster)
# - `dtm` is your Document-Term Matrix (sparse or dense)

# Convert DTM to a dataframe with cluster labels
term_importance <- as.data.frame(as.matrix(BoW_lda_dtm)) %>%
  mutate(cluster = function_df$BoW_k_means_cluster) %>%               # Add cluster labels
  group_by(cluster) %>%
  summarise(across(everything(), mean)) %>%    # Mean term frequency per cluster
  pivot_longer(-cluster, names_to = "term", values_to = "mean_tfidf") %>%
  group_by(cluster) %>%
  slice_max(mean_tfidf, n = 10) %>%           # Top 10 terms per cluster
  arrange(cluster, desc(mean_tfidf))

# Visualize
cluster_comp <- ggplot(term_importance, aes(x = reorder(term, mean_tfidf), y = mean_tfidf, fill = factor(cluster))) +
  geom_col() +
  coord_flip() +
  facet_wrap(~cluster, scales = "free_y") +
  labs(x = "Term", y = "Mean TF-IDF Weight", title = "Key Terms per Cluster")

print(cluster_comp)
```

```{r}
# Get document-topic probabilities (gamma matrix)
doc_topics <- tidy(BoW_lda_model, matrix = "gamma")

# Calculate document-document similarity
doc_sim_matrix <- doc_topics %>%
  pivot_wider(names_from = topic, values_from = gamma) %>%
  column_to_rownames("document") %>%
  as.matrix()
  #z Transpose for doc-doc similarity
doc_sim_matrix <-   lsa::cosine(t(doc_sim_matrix))  # Cosine similarity
# Create document-document edges
tt_edges <- doc_sim_matrix %>%
  as.data.frame() %>%
  tibble::rownames_to_column("from") %>%
  pivot_longer(-from, names_to = "to", values_to = "weight") %>%
  filter(from < to & weight > 0.2) %>%
  mutate(type = "doc-doc")

# Get top documents per topic
topic_docs <- doc_topics %>%
  group_by(topic) %>%
  slice_max(gamma, n = 100) %>%
  ungroup()

# Get top terms per topic
topic_terms <- tidy(BoW_lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>%
  ungroup()

# Create document-term edges
dt_edges <- topic_docs %>%
  inner_join(topic_terms, by = "topic") %>%
  select(from = document, to = term, topic, weight = gamma) %>%
  mutate(type = "doc-term")

# Combine all edges
edges <- bind_rows(tt_edges, dt_edges) %>%
  mutate(weight = ifelse(is.na(weight), 1, weight))  # Default weight for doc-doc

# Create nodes data
nodes <- data.frame(
  name = unique(c(edges$from, edges$to)),
  type = ifelse(unique(c(edges$from, edges$to)) %in% topic_docs$document,
                "document", "term")
) %>%
  left_join(
    bind_rows(
      doc_topics %>%
        group_by(document) %>%
        slice_max(gamma, n = 1) %>%
        select(name = document, topic),
      topic_terms %>%
        group_by(term) %>%
        slice_max(beta, n = 1) %>%
        select(name = term, topic)
    ),
    by = "name"
  )

# Create graph
doc_term_graph <- tbl_graph(
  nodes = nodes,
  edges = edges,
  directed = TRUE
)

# Plot
function_lda_plot <- ggraph(doc_term_graph, layout = "fr") + #, circular = TRUE) +
  # Document-term edges
  geom_edge_link(
    aes(filter = type == "doc-term", alpha = weight*0.1),
    color = "gray70",
    width = 0.3
  ) +
  # Document-document edges
  # geom_edge_link(
  #   aes(filter = type == "doc-doc", width = weight*0.01),
  #   color = "orange",
  #   alpha = 0.3
  # ) +
  # Nodes
  geom_node_point(
    aes(color = as.factor(topic),
        size = ifelse(type == "term", 5, 3L
        ),
    alpha = 0.8),
    show.legend = NULL
  ) +
  # Labels
  geom_node_text(
    aes(label = ifelse(type == "term", name, "")),
    color = "black",
    size = 3,
    fontface = "bold",
    repel = TRUE,
    show.legend = NULL
  ) +
  # Hulls
  geom_mark_hull(
    aes(x, y, group = topic, fill = as.factor(topic)),
    concavity = 1,
    expand = unit(2, "mm"),
    alpha = 0.05,
    size = 0.2,
    show.legened = NULL
  ) +
  scale_edge_width(range = c(0.5, 2)) +
  labs(title = "Combined Document Network") +
  theme_void()

ggsave("/Users/alec/Documents/GradSchool/Yeung_Lab/GroupMeetings/GroupMeetingPresentation2025_05_16 /Slides/Figures/function_lda_plot.png",
       dpi=600,
       height=7.5,
       width=10)

print(function_lda_plot)
```

```{r}
# Preprocess data more efficiently
preprocess_data <- function() {
  function_df <- read.csv("../Data/Function_Descriptions.csv",
                         stringsAsFactors = FALSE)
  heatmap_data <- read.csv("../Data/Heatmap_Data.csv",
                          stringsAsFactors = FALSE)

  corpus <- tm::Corpus(tm::VectorSource(function_df$GO)) %>%
    tm::tm_map(tm::removePunctuation) %>%
    tm::tm_map(tm::removeWords, tm::stopwords("en")) %>%
    tm::tm_map(tm::stemDocument)

  function_df$GO_corpus <- sapply(corpus, as.character)

  Master_df <- merge(function_df, heatmap_data, by = "Gene", all = FALSE) %>%
    na.omit() %>%
    mutate(identity = paste(Gene, genotype))

  return(Master_df)
}

Master_df <- preprocess_data()
write.csv(Master_df, file = "../Data/Master_df.csv", row.names = FALSE)
```
```{r}
Master_df$identity <- paste(Master_df$Gene, Master_df$genotype)
Master_df

Master_df$identity <- paste(Master_df$Gene, Master_df$genotype)
Master_df
```

```{r}
source("optimized_lda.R")
# Initialize analyzer with validation
analyzer <- OptimizedLDAAnalyzer$new(
  master_df = Master_df,
  go_corpus_col = "GO"  # Column containing GO terms
)
```

```{r}
source("optimized_lda.R")
# Initialize analyzer with validation
analyzer <- OptimizedLDAAnalyzer$new(
  master_df = Master_df,
  go_corpus_col = "GO"  # Column containing GO terms
)
# Debug
analyzer$validate()
analyzer$master_df
analyzer$go_corpus_col
analyzer$get_valid_go()
analyzer$prepare_dtm()
analyzer$find_optimal_k()
analyzer$fit_lda()
analyzer$build_genotype_network()
analyzer$build_topic_network()
analyzer$build_topic_document_network()
analyzer$plot_topic_document_graph()
analyzer$plot_topic_graph()
```
```{r}
dtm <- analyzer$dtm
k <- analyzer$optimal_k
lda_model <- analyzer$lda_model

top_doc_network <- analyzer$build_topic_document_network(gamma_threshold = 0.1)
topic_network <- analyzer$build_topic_network()
doc_doc_network <- analyzer$build_document_network()
combined_network <- analyzer$combine_networks(gamma_threshold = 0.1,
                                              dd_gamma_threshold = 0.95)

combined_network <- combined_network %>%
  activate(nodes) %>%
  filter(group != "WT")
#organized_layout <- analyzer$organize_nodes()

# Calculate the gene (name) difference between groups s4 and NC.
# This will help identify the genes that are unique to each group

# Calculate |s4|/|NC| ratios 
diff_combined_nodes <- combined_network_nodes %>%
  pivot_wider(
    id_cols = name,
    names_from = group,
    values_from = log2FoldChange
  ) %>%
  filter(!is.na(s4) & !is.na(NC)) %>%
  mutate(
    group = "|s4|/|NC|",
    log2FoldChange = abs(s4)/abs(NC)
  ) %>%
  select(name, group, log2FoldChange)

test_ratio <- diff_combined_nodes

# Add ratio nodes to graph
diff_combined_network <- combined_network %>%
  bind_nodes(diff_combined_nodes)
test_2 <- diff_combined_network %>%
  activate(nodes) %>%
  as_tibble()
# Get node indices using node positions
s4_nodes <- diff_combined_network %>% 
  activate(nodes) %>% 
  as_tibble() %>% 
  filter(group == "s4" & name %in% diff_combined_nodes$name) %>% 
  rownames() %>% 
  as.numeric()

ratio_nodes <- diff_combined_network %>% 
  activate(nodes) %>% 
  as_tibble() %>% 
  filter(group == "|s4|/|NC|") %>% 
  rownames() %>% 
  as.numeric()

# First get the edge types from original s4 connections
original_edge_types <- combined_network %>%
  activate(edges) %>%
  as_tibble() %>%
  # Get edges connected to our s4 nodes
  filter(to %in% which(combined_network %>% activate(nodes) %>% 
                      as_tibble() %>% 
                      filter(group == "s4") %>% 
                      pull(name) %in% diff_combined_nodes$name)) %>%
  select(to, original_type = type)

# Create a mapping between s4 node indices and their original edge types
type_mapping <- original_edge_types %>%
  distinct(to, original_type) %>%
  deframe()  # Convert to named vector

# Add edges safely
diff_combined_network <- diff_combined_network %>%
  bind_edges(
    tibble(
      from = ratio_nodes,
      to = s4_nodes,
      type = coalesce(type_mapping[as.character(s4_nodes)], "gene_diff"),
      weight = 1 # Optional: add edge weights
    )
  )

test3 <- diff_combined_network %>%
  activate(nodes) %>%
  as_tibble()

# Create tibbles to easily debug networks
top_doc_nodes <- top_doc_network %>%
  activate(nodes) %>%
  as_tibble()
top_doc_edges <- top_doc_network %>%
  activate(edges) %>%
  as_tibble()
doc_doc_nodes <- doc_doc_network %>%
  activate(nodes) %>%
  as_tibble()
doc_doc_edges <- doc_doc_network %>%
  activate(edges) %>%
  as_tibble()
top_top_nodes <- topic_network %>%
  activate(nodes) %>%
  as_tibble()
top_top_edges <- topic_network %>%
  activate(edges) %>%
  as_tibble()
combined_network_nodes <- combined_network%>%
  activate(nodes) %>%
  as_tibble()
combined_network_edges <- combined_network %>%
  activate(edges) %>%
  as_tibble()
combined_graph_edges <- combined_graph %>%
  activate(edges) %>%
  as_tibble()
combined_graph_nodes <- combined_graph %>%
  activate(nodes) %>%
  as_tibble()
diff_combined_network_nodes <- diff_combined_network %>%
  activate(nodes) %>%
  as_tibble()
diff_combined_network_edges <- diff_combined_network %>%
  activate(edges) %>%
  as_tibble()

# create updated network (ChatGPT generated)

# Step 1: Separate gene nodes (type == 'document') and topic nodes
gene_nodes <- diff_combined_network_nodes %>% filter(type == "document")
topic_nodes <- diff_combined_network_nodes %>% filter(type != "document")

# Step 2: Aggregate gene nodes by 'name'
agg_gene_nodes <- gene_nodes %>%
  group_by(name) %>%
  summarise(
    across(where(is.numeric) & !c("node_size"), mean, na.rm = TRUE),
    group = first(group),
    type = first(type),
    node_type = first(node_type),
    .groups = 'drop'
  ) %>%
  mutate(node_size = 2.0)  # Assign constant or derived node size

# Step 3: Combine with topic nodes and assign new node IDs
new_nodes <- bind_rows(agg_gene_nodes, topic_nodes) %>%
  mutate(new_id = row_number())

# Step 4: Map original node row numbers to names
original_node_map <- diff_combined_network_nodes %>%
  mutate(orig_id = row_number()) %>%
  select(orig_id, name)

# Create name-to-new ID mapping
name_to_new_id <- new_nodes %>%
  select(name, new_id, log2FoldChange
         )

# Step 5: Remap edge node indices based on name
updated_edges <- diff_combined_network_edges %>%
  left_join(original_node_map, by = c("from" = "orig_id")) %>%
  rename(from_name = name) %>%
  left_join(original_node_map, by = c("to" = "orig_id")) %>%
  rename(to_name = name) %>%
  left_join(name_to_new_id, by = c("from_name" = "name")) %>%
  rename(new_from = new_id) %>%
  left_join(name_to_new_id, by = c("to_name" = "name")) %>%
  rename(new_to = new_id) %>%
  filter(!is.na(new_from) & !is.na(new_to)) %>%
  mutate(from = new_from, to = new_to) %>%
  select(from, to, everything(), -new_from, -new_to, -from_name, -to_name)

# Step 6: Build the new tidygraph object
diff_combined_network_aggregated <- tbl_graph(
  nodes = new_nodes %>% select(-new_id),
  edges = updated_edges,
  directed = TRUE
)

diff_combined_network_aggregated <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  mutate(
    community = case_when(
      type == "topic" ~ as.numeric(str_extract(name, "\\d+"))/10,  # Extract topic numbers
      TRUE ~ NA_real_  # Documents remain unassigned initially
    )
  )

diff_combined_network_aggregated_nodes <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  as_tibble()
diff_combined_network_aggregated_edges <- diff_combined_network_aggregated %>%
  activate(edges) %>%
  as_tibble()

rcc_network <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  mutate(
    community = ifelse(
      type == "document",
      # Find the topic with highest edge weight for each document
      edge_weights_to_topics <- .E()$weight[.N()$type == "topic"],
      community  # Keep existing topic communities
    )
  )

# # Option B: Use community detection that respects topic assignments (Louvain with constraints)
# rcc_network <- rcc_network %>%
#   mutate(
#     community = group_louvain(weights = weight, constraints = ifelse(type == "topic", community, NA))
#   )
# Modify the nodes to cluster around the topic centers through definition of comunnity based on similarity using fuzzy community membership.
rcfc_network <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  mutate(
    dominant_community = ifelse(
      type == "document",
      map_int(row_number(), ~{
        # Get all connected topics
        topics <- .E() %>%
          as_tibble() %>%
          filter(from == .x & .N()$type[to] == "topic") %>%
          pull(to)

        if (length(topics) == 0) {
          NA_integer_  # Return NA if no topic connections
        } else {
          # Find topic with highest average weight
          weights <- .E()$weight[.E()$from == .x & .E()$to %in% topics]
          topics[which.max(weights)]
        }
      }),
      community  # Keep original community for topics
    )
  )

# Visualizes
ggraph(combined_network, layout = "kk") +
  geom_edge_link(aes(alpha = weight, width = weight),#, color = edge_color),
                 show.legend = FALSE) +
  geom_node_point(aes(color = log2FoldChange, shape = group)) +
  scale_edge_alpha(range = c(0.1, 0.5)) +
  scale_edge_width(range = c(0.2, 1.5)) +
  theme_graph() +
  labs(size = "Centrality", color = "Group") +
  guides(edge_alpha = "none", edge_width = "none")

rcc_network <- rcc_network %>%
  activate(nodes) %>%
  mutate(
    ratio_test = ifelse(log2FoldChange > 1, "s4", "NC")
  )

# Create tibbles for the difference combined network with fuzzy community (rcfc) membership
rcc_network_nodes <- rcc_network %>%
  activate(nodes) %>%
  as_tibble()
rcc_network_edges <- rcc_network %>%
  activate(edges) %>%
  as_tibble()

rcfc_network_nodes <- rcfc_network %>%
  activate(nodes) %>%
  as_tibble()
rcfc_network_edges <- rcfc_network %>%
  activate(edges) %>%
  as_tibble()

topic_size <- 5 #`alpha`
ggraph(rcc_network, layout = "drl") + #, pivots = 10) + #, weight = weight) +#, group = "community") + #, circular=TRUE) +
  geom_edge_link(
    aes(filter = type == "document_document",
        alpha = weight*0.2,
        color = "Document-Document"),
    width = 0.1
  ) +
  geom_edge_link(
    aes(filter = type == "topic_document",
        alpha = weight,
        color = "Document-Topic"),
    width = 0.5,
    linetype = "dashed"
  ) +
  geom_edge_link(
    aes(filter = type == "term_term",
        alpha = weight,
        color = "Topic-Topic"),
    width = 0.9
  ) +
  geom_node_point(
    aes(color = ratio_test, size = type)
  ) +
  scale_color_manual(
    name = "ratio_test",
    values = c("s4" = "firebrick", "NC" = "darkblue") #c("document" = "darkblue", "topic" = "firebrick")
  ) +
  scale_edge_color_manual(
    name = "Edge Type",
    values = c("Document-Document" = "darkblue", "Topic-Topic" = "firebrick", 
               "Document-Topic" = "darkgreen")
  ) +
  geom_node_text(aes(label = ifelse(type == "topic", funcs, "")),
            color = "black",
            size = 3,
            fontface = "bold",
            repel = TRUE,
            show.legend = FALSE) +
  theme_graph() +
  labs(title = "Multiplex Network: Documents, Topics, and Terms")
```


```{r}
library(tidygraph)
library(ggraph)

# 1. Identify most significant topic for each gene
# 1. Get node indices mapping
# 1. First create a node index lookup table
node_index_map <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  as_tibble() %>%
  mutate(node_id = row_number()) %>%
  select(name, node_id)

# 2. Process gene-topic importance using indices
gene_topic_importance <- diff_combined_network_aggregated %>%
  activate(edges) %>%
  as_tibble() %>%
  filter(type == "document-topic") %>%
  # Convert node names to indices
  left_join(node_index_map, by = c("from" = "name")) %>%
  rename(gene_index = node_id) %>%
  left_join(node_index_map, by = c("to" = "name")) %>%
  rename(topic_index = node_id) %>%
  # Find primary topic for each gene
  group_by(gene_index) %>%
  top_n(1, weight) %>%
  select(gene_index, main_topic_index = topic_index)

# 3. Create layout using indices
custom_layout <- diff_combined_network_aggregated %>%
  activate(nodes) %>%
  # Add node index
  mutate(node_index = row_number()) %>%
  # Join with gene-topic importance
  left_join(gene_topic_importance, by = c("node_index" = "gene_index")) %>%
  # Add topic center positions
  left_join(
    topic_centers %>%
      mutate(topic_index = row_number()),
    by = c("main_topic_index" = "topic_index")
  ) %>%
  mutate(
    x = ifelse(type == "topic", x_center, x_center + rnorm(n(), 0, 0.2)),
    y = ifelse(type == "topic", y_center, y_center + rnorm(n(), 0, 0.2))
  )

```

```{r}
setdiff(unique(c(edges$from, edges$to)), nodes$name)
```

```{r}
analyzer$doc_topic_edges
```
```{r}
analyzer$term_term_edges
```

```{r}
combined_network
topic_network
top_doc_network
```

```{r}
analyzer$doc_topics
```
```{r}
all_go_ids <- unique(unlist(strsplit(Master_df$GO, split = ",\\s*")))
  all_go_ids <- all_go_ids[!is.na(all_go_ids)]

  go_terms <- tryCatch({
    data.frame(
      GOID = all_go_ids,
      Term = AnnotationDbi::Term(all_go_ids)
    )
  }, error = function(e) {
    warning("GO term mapping failed: ", e$message)
    data.frame(GOID = all_go_ids, Term = all_go_ids)
  })

top_terms <- data.frame(lapply(top_terms, gsub, pattern = "go:", replacement = "GO:"))
top_terms <- top_terms %>%
  mutate(
    Term = go_terms$Term[match(terms, go_terms$GOID)]
  )
```
# Key Centrality Methods
## Function	Biological Interpretation	Use Case
 - centrality_degree()	Counts connections to other topics.	Identify hub topics
 - centrality_betweenness()	Detects topics bridging different clusters.	Find integrative biological themes
 - centrality_pagerank()	Measures influence through network flows.	Rank topics by importance
 - centrality_eigen()	Scores well-connected neighbor influence.	Find core regulatory processes
```{r}
library(ggraph)
library(tidygraph)
library(patchwork)
library(plotly)

plot_centrality <- function(network, layout = "fr", label_quantile = 0.75) {
  # Create consistent theme
  net_theme <- function() {
    theme_void() +
      theme(
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.margin = margin(5, 5, 5, 5)
      )
  }

  # Helper function to create individual plots
  create_centrality_plot <- function(network, size_var, color_var, title) {
    ggraph(network, layout = layout) +
      geom_edge_link(aes(alpha = weight),
                    color = "grey85",
                    show.legend = FALSE) +
      geom_node_point(aes(size = !!sym(size_var),
                         color = !!sym(color_var),
                         text = paste(
                           "Topic:", topic, "\n",
                           "Degree:", round(degree, 2), "\n",
                           "Betweenness:", round(betweenness, 2), "\n",
                           "PageRank:", round(page_rank, 2), "\n",
                           "Eigen:", round(eigen, 2)
                         ))) +
      geom_node_label(
        aes(label = ifelse(degree > quantile(degree, label_quantile),
                          paste("Topic", topic), ""),
            filter = degree > quantile(degree, label_quantile)),
        repel = TRUE,
        size = 3,
        label.padding = unit(0.15, "lines")
      ) +
      scale_size_continuous(range = c(3, 8)) +
      scale_color_viridis_c(option = "plasma") +
      labs(title = title) +
      net_theme()
  }

  # Generate all plot combinations
  plots <- list(
    create_centrality_plot(network, "degree", "betweenness", "Degree vs Betweenness"),
    create_centrality_plot(network, "degree", "page_rank", "Degree vs PageRank"),
    create_centrality_plot(network, "degree", "eigen", "Degree vs Eigen Centrality"),
    create_centrality_plot(network, "betweenness", "page_rank", "Betweenness vs PageRank"),
    create_centrality_plot(network, "betweenness", "eigen", "Betweenness vs Eigen"),
    create_centrality_plot(network, "page_rank", "eigen", "PageRank vs Eigen")
  )

  # Combine with patchwork
  combined <- wrap_plots(plots, ncol = 3) +
    plot_annotation(
      title = "Topic Network Centrality Analysis",
      subtitle = "Node size represents first metric, color represents second metric",
      theme = theme(
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 12, hjust = 0.5)
      )
    )

  # Return both static and interactive versions
  list(
    static = combined,
    interactive = ggplotly(plots[[1]], tooltip = "text")  # First plot as interactive
  )
}

# Usage:
viz <- plot_centrality(topic_network, layout = "stress")
print(viz$static)  # Print beautiful static version
viz$interactive    # Explore interactive version
```

```{r}
library(ggraph)
library(tidygraph)
library(ggplot2)
library(plotly)
Sys.setlocale(category = "LC_ALL", locale = "en_US.UTF-8")  # Linux/macOS

plot_network <- function(network, layout = "fr",
                         min_edge_alpha = 0.2,
                         label_quantile = 0.9,
                         top_terms = NULL) {
    # Check if input is a tbl_graph; if not, try to convert it

    # 1. Extract node data to verify genotypes
  node_data <- network %>% activate(nodes) %>% as_tibble()

  # 2. Ensure genotype is a factor with valid levels
  if ("genotype" %in% names(node_data)) {
    network <- network %N>%
      mutate(
        genotype = factor(genotype, levels = c("WT", "NC", "s4")),  # Adjust levels as needed
        shape = as.numeric(genotype)  # Map to numeric shape codes
      )
  } else {
    network <- network %N>% mutate(shape = 16)  # Default circle
  }
  # Normalize metrics for visualization

  network <- network %N>%
    mutate(
      #size = log2FoldChange,
      size = scales::rescale(centrality_alpha(), to = c(3, 10)),
      color = log2FoldChange
      #color = scales::rescale(eigen, to = c(0, 1))
    ) %E>%
    mutate(
      weight = scales::rescale(weight, to = c(min_edge_alpha, 1))
    )

  if (!is.null(top_terms)) {
    network <- network %N>%
      mutate(
        name = top_terms$Term[match(topic, top_terms$topic)]
      )} else if ("topic" %in% (network %>% activate(nodes) %>% as_tibble() %>% names()))  {
      network <- network %N>%
        mutate(
          name = topic
        )} else if ("document" %in% (network %>% activate(nodes) %>% as_tibble() %>% names())) {
      network <- network %N>%
        mutate(
          name = document
        )} else if ("term" %in% colnames(network)) {lll
    } else {
      stop("No valid topic column found in the network.")
    }

  print(network)
  # Create the plot
  p <- ggraph(network, layout = layout) +
    # Edges - alpha mapped to weight
    geom_edge_bend0(
      aes(alpha = weight),
      color = "black",
      width = 0.8,
      show.legend = TRUE
    ) +
    # Nodes - size to degree, color to eigen centrality
    geom_node_point(
      aes(size = size,
          color = color,
          shape = factor(genotype)
      ),
      alpha = 0.8,
      show.legend = TRUE
    ) +
    # Labels for top central nodes
    geom_node_label(
      aes(
      label =  name),
      #filter = centrality_alpha > quantile(centrality_alpha, label_quantile, na.rm = TRUE),
      repel = TRUE,
      size = 3,
      fill = alpha("white", 0.6),
      label.size = 0.1,
      label.padding = unit(0.1, "lines")
    ) +
    # Visual refinements
    scale_size_identity() +
    scale_color_viridis_c(
      option = "plasma",
      guide = guide_colorbar(
        title = "Eigen Centrality",
        barwidth = unit(1, "cm")
      )
    ) +
    scale_edge_alpha(
      name = "Edge Weight",
      guide = guide_legend(
        title.position = "top",
        label.position = "bottom")
    ) +
    labs(
      title = "Topic Network Analysis",
      subtitle = "Node size ~ Alpha Centrality | Color ~ Eigen Centrality | Edge transparency ~ Edge Weight"
    ) +
    theme_graph(base_family = "sans") +
    theme(
      legend.position = "bottom",
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 10)
    )
  return(p)
}

topic_network_tibble = as.tibble(topic_network)
# Filter out WT nodes and their edges
filtered_topic_network <- genotype_network %>%
  activate(nodes) %>%               # Focus on nodes
  filter(genotype != "WT") %>%      # Keep only non-WT nodes
  activate(edges) %>%               # Automatically drops disconnected edges
  filter(!edge_is_incident("WT"))   # Explicit edge removal (optional)


# Plot with alpha centrality
p1 <- plot_network(genotype_network, layout = "kk")#, top_terms=top_terms)  # Use Kamada-Kawai layout
p2 <- plot_network(filtered_topic_network, layout = "kk")  # Use Kamada-Kawai layout

plots <- p1 + p2

library(patchwork)
combined_plot <- wrap_plots(plots, ncol = 2) +
  plot_annotation(title = "Comparison of Gene Sorting Methods") &
  scale_fill_gradient2(low = "blue", mid = "white", high = "red") &
  theme_minimal()
#print(combined_plot)

ggsave("combined_plot.pdf", combined_plot, width = 40, height = 10)

print(p)
```